{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02633672",
   "metadata": {},
   "source": [
    "<a id = \"toc\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "* [1. Import the Libraries](#chapter1)\n",
    "* [2. Import the Datasets](#chapter2)\n",
    "* [3. Explore the Dataset ](#chapter3)\n",
    "    * [3.1. Basic Statistics](#section_3_1)\n",
    "    * [3.2. Inconsistencies](#section_3_2)\n",
    "        * [3.2.1. Checking Combinations of Code](#sub_section_3_2_1)\n",
    "        * [3.2.2. Handling Covid-19 Inconsistencies](#sub_section_3_2_2)\n",
    "        * [3.2.3. Handling Average Weekly Wage Inconsistencies](#sub_section_3_2_3)\n",
    "        * [3.2.4. Handling Birth Year Inconsistencies](#sub_section_3_2_4)\n",
    "        * [3.2.5. Age at Injury vs. Birth Year](#sub_section_3_2_5)\n",
    "        * [3.2.6. Age at Injury](#sub_section_3_2_6)\n",
    "        * [3.2.7. First Hearing Date vs. Accident Date](#sub_section_3_2_7)\n",
    "        * [3.2.8. C2 Date vs. C3 Date vs. Accident Date](#sub_section_3_2_8)\n",
    "        * [3.2.9. Assembly Date vs. Accident Date](#sub_section_3_2_9)\n",
    "        * [3.2.10. Handling ZIP Code Format](#sub_section_3_2_10)\n",
    "        * [3.2.11. Overview of Inconsistencies](#sub_section_3_2_11)\n",
    "* [4. Visual Exploration ](#chapter4)\n",
    "    * [4.1. Univariate Plots](#section_3_3)  \n",
    "        * [4.1.1. Continuous Variables](#sub_section_4_1_1)\n",
    "        * [4.1.2. Categorical Variables](#sub_section_4_1_2)\n",
    "        * [4.1.3. Discrete Variables](#sub_section_4_1_3)\n",
    "        * [4.1.4. Binary Variables](#sub_section_4_1_4)\n",
    "    * [4.2. Multivariate Analysis](#section_4_2)  \n",
    "* [5. Save Dataset for Preprocessing](#chapter5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca0aa4",
   "metadata": {},
   "source": [
    "# 1. Import the Libraries <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d8b98a",
   "metadata": {},
   "source": [
    "# 2. Load and Prepare Datasets <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c529e5c",
   "metadata": {},
   "source": [
    "Inconcistencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e14841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace negative code (-9) with 101 and update description to 'Nonclassifiable'\n",
    "df_train.loc[df_train['WCIO Part Of Body Code'] < 0, 'WCIO Part Of Body Code'] = 101\n",
    "df_train.loc[df_train['WCIO Part Of Body Code'] == 101, 'WCIO Part Of Body Description'] = 'Nonclassifiable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7adb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in the target column ('Claim Injury Type')\n",
    "df_train_cleaned = df_train_cleaned.dropna(subset=['Claim Injury Type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72efe81",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933b2f26",
   "metadata": {},
   "source": [
    "Basic plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e2be2",
   "metadata": {},
   "source": [
    "Continuas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aeefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the dataset to remove rows with 'Average Weekly Wage' equal to zero and extreme values(using the quantile 0.95)\n",
    "filtered_df = df[(df['Average Weekly Wage'] > 0) & (df['Average Weekly Wage'] < df['Average Weekly Wage'].quantile(0.95))]\n",
    "\n",
    "# Set figure size and plot pairplot\n",
    "g = sns.pairplot(filtered_df, vars=['Average Weekly Wage'], hue='Claim Injury Type', palette='Set2', height=5, aspect=1.5)\n",
    "\n",
    "# Add a title to the pairplot\n",
    "g.fig.suptitle(\"Pairplot between continuous variable and target (filtered data)\", y=1.02)\n",
    "\n",
    "# Manually add legend outside of plot\n",
    "plt.legend(\n",
    "    labels=filtered_df['Claim Injury Type'].unique(),\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc='upper left',\n",
    "    fontsize='medium'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b3952",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))  # Increased figure size for better visibility\n",
    "sns.boxplot(data=filtered_df, x='Claim Injury Type', y='Average Weekly Wage', palette='Set2')\n",
    "plt.title(f'Distribution of Average Weekly Wage by Claim Injury Type')\n",
    "plt.xlabel('Claim Injury Type')  # Adding labels for better clarity\n",
    "plt.ylabel('Average Weekly Wage')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903e6f5",
   "metadata": {},
   "source": [
    "Categoricas e variaveis discretas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaea2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Columns: Plotting Frequency Distributions with Target as Hue\n",
    "# The Categorical Column was created above.\n",
    "\n",
    "# Loop through categorical columns and create separate figures\n",
    "for column in categorical_columns:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot using seaborn to include hue (target column)\n",
    "    ax = sns.countplot(data=df, x=column, hue='Claim Injury Type', palette='viridis', order=df[column].value_counts().iloc[:10].index)\n",
    "    \n",
    "    # Set title and labels\n",
    "    plt.title(f'Top 10 Most Frequent Values in {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add count labels above each bar\n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{int(height)}', (p.get_x() + p.get_width() / 2., height),\n",
    "                        ha='center', va='bottom', fontsize=8, color='black')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83f7f9",
   "metadata": {},
   "source": [
    "Ver binomiais "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to exclude\n",
    "exclude_columns = ['Zip Code', 'Carrier Name']  # This columns have many elements to evalute, so we are going to exclude.\n",
    "\n",
    "#  Get binary variables from the dataframe\n",
    "binary_vars = [col for col in binary_columns if col not in exclude_columns]  # Identify binary variables\n",
    "\n",
    "# Get categorical variables excluding the specified columns\n",
    "categorical_vars = [col for col in categorical_columns if col not in exclude_columns]\n",
    "\n",
    "   \n",
    "# Generate count plots for binary and categorical variables\n",
    "plot_count_for_binary_and_categorical(df, binary_vars, categorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_for_binary_and_discrete(data, binary_vars, discrete_vars):\n",
    "    for discrete_var in discrete_vars:\n",
    "        print(f\"Discrete Variable: {discrete_var}\\n\")  # Print the binary variable being plotted\n",
    "        \n",
    "        # Loop through each categorical variable\n",
    "        for binary_var in binary_vars:\n",
    "            plt.figure(figsize=(16, 8))  # Increase figure size for better clarity\n",
    "            \n",
    "            # Create the count plot with aesthetics for categorical variables\n",
    "            ax = sns.countplot(\n",
    "                data=data, \n",
    "                x=binary_var, \n",
    "                hue=discrete_var, \n",
    "                palette=\"muted\", \n",
    "                order=data[binary_var].value_counts().index,  # Order categories by frequency\n",
    "                linewidth=0.5, \n",
    "                edgecolor=\"gray\"\n",
    "            )\n",
    "            \n",
    "            # Add annotations to display counts on top of the bars\n",
    "            for container in ax.containers:\n",
    "                ax.bar_label(container, fmt='%d', label_type='edge', fontsize=8, padding=3)\n",
    "            \n",
    "            # Improve titles and labels for clarity\n",
    "            plt.title(f\"Distribution of {discrete_var} by {binary_var}\", fontsize=14, fontweight='bold')  \n",
    "            plt.xlabel(binary_var, fontsize=12)  \n",
    "            plt.ylabel(\"Count\", fontsize=12)  \n",
    "            plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for readability\n",
    "            \n",
    "            # Optimize layout to avoid overlapping elements\n",
    "            plt.tight_layout()  \n",
    "            plt.legend(title=discrete_var, loc='upper right')  # Add legend\n",
    "            \n",
    "            plt.show()  # Display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_feature_with_proportion(X, y, feature, target_name='Claim Injury Type', top_n=10):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    \n",
    "    # Combine feature and target into a new DataFrame\n",
    "    data = X[[feature]].copy()\n",
    "    data[target_name] = pd.Series(y, index=X.index)  # Ensure alignment\n",
    "    \n",
    "    # Filter the top N most frequent values for the feature\n",
    "    top_values = data[feature].value_counts().head(top_n).index\n",
    "    filtered_data = data[data[feature].isin(top_values)]\n",
    "    \n",
    "    # Calculate proportions of target values for each feature value\n",
    "    proportion_data = (\n",
    "        filtered_data.groupby([feature, target_name])\n",
    "        .size()\n",
    "        .groupby(level=0)\n",
    "        .apply(lambda x: x / x.sum())\n",
    "        .reset_index(name='Proportion')\n",
    "    )\n",
    "    \n",
    "    # Pivot for stacked bar plot\n",
    "    pivot_data = proportion_data.pivot(index=feature, columns=target_name, values='Proportion').fillna(0)\n",
    "    pivot_data = pivot_data.loc[top_values]  # Ensure correct order\n",
    "    \n",
    "    # Plot the stacked bar plot\n",
    "    fig, ax = plt.subplots(figsize=(18, 12))\n",
    "    pivot_data.plot(kind='bar', stacked=True, ax=ax, colormap='viridis')\n",
    "    \n",
    "    # Annotate the bars with percentage values\n",
    "    for i, bar_group in enumerate(ax.containers):  # Loop through stacked bars\n",
    "        for bar in bar_group:\n",
    "            height = bar.get_height()\n",
    "            if height > 0:  # Only annotate non-zero segments\n",
    "                ax.annotate(f'{height:.0%}',  # Convert to percentage\n",
    "                            (bar.get_x() + bar.get_width() / 2, bar.get_y() + height / 2),\n",
    "                            ha='center', va='center', fontsize=13, color='white', weight='bold')\n",
    "    \n",
    "    # Final plot adjustments\n",
    "    plt.title(f\"Top {top_n} Most Frequent {feature} with {target_name} Proportions\")\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    plt.legend(title=target_name, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bdf85b",
   "metadata": {},
   "source": [
    "# Feature Engineering & Encoding Notebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c72a31",
   "metadata": {},
   "source": [
    "fazer bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b773843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the bins and labels for categorizing income based on percentiles\n",
    "income_bins = [0, 876.0, 1125.0, 1269.0, 1666.0, float('inf')]  # float('inf') allows us to set an open-ended range\n",
    "income_labels = ['Low Income', 'Lower-Middle Income', 'Middle Income', 'Upper-Middle Income', 'High Income']\n",
    "\n",
    "# Creating the new feature for income categories for the train set\n",
    "X_train['Income_Category'] = pd.cut(X_train['Average Weekly Wage'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Apply to the val set\n",
    "X_val['Income_Category'] = pd.cut(X_val['Average Weekly Wage'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "# Apply to the test set\n",
    "df_test['Income_Category'] = pd.cut(df_test['Average Weekly Wage'], bins=income_bins, labels=income_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08cc29",
   "metadata": {},
   "source": [
    "Encoding\n",
    "- label_encoder = LabelEncoder()\n",
    "- freq e ohe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7408ace",
   "metadata": {},
   "source": [
    "Devemos incluir binarias? Spearman Correlation Matrix for Numerical and Binary Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6906d",
   "metadata": {},
   "source": [
    "Escolher com base na interpretabilidade pode ser um criterio. \n",
    "\n",
    "Escolher com base no que é original?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef27c48",
   "metadata": {},
   "source": [
    "### LASSO Regression <a class=\"anchor\" id=\"sub_section_4_2_1\"></a>\n",
    "\n",
    "USAMOS MESMO QUANDO NAO E LINEAR?\n",
    "\n",
    "\n",
    " The LASSO (Least Absolute Shrinkage and Selection Operator) regression is used here for feature selection by fitting a model to the standardized dataset and analyzing the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99348d3",
   "metadata": {},
   "source": [
    " ### Recursive Feature Elimination - RFE <a class=\"anchor\" id=\"sub_section_4_2_2\"></a>\n",
    "\n",
    "\n",
    "RFE is employed here to further validate the important features as identified by LASSO. By sequentially removing the least important features, RFE helps to refine the feature set.\n",
    "\n",
    "The selected features after RFE likely overlap with those identified by LASSO, suggesting consistency in feature importance.\n",
    "Using both LASSO and RFE provides a more robust feature selection by cross-validating the importance of individual features.\n",
    "\n",
    "This block of code performs RFE to identify the best subset of features by iterating over a range of feature numbers. The code aims to maximize model performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac0b687",
   "metadata": {},
   "source": [
    "### Feature Importance - Decision Tree <a class=\"anchor\" id=\"sub_section_4_2_3\"></a>\n",
    "### Feature Importance - Random Forest <a class=\"anchor\" id=\"sub_section_4_2_4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce775735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This function is it to find the optimal number of features using Recursive Feature Elimination (RFE) and evaluates using F1 Macro score.\n",
    "def find_optimal_features_with_rfe(model, X_train, y_train, X_val, y_val, max_features=8):\n",
    "    \"\"\"\n",
    "    Finds the optimal number of features using Recursive Feature Elimination (RFE) \n",
    "    and evaluates using F1 Macro score.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The machine learning model (e.g., LogisticRegression()).\n",
    "    - X_train: Scaled training feature set (numpy array or DataFrame).\n",
    "    - y_train: Encoded training target labels.\n",
    "    - X_val: Scaled validation feature set (numpy array or DataFrame).\n",
    "    - y_val: Encoded validation target labels.\n",
    "    - max_features: Maximum number of features to evaluate (default=8).\n",
    "\n",
    "    Returns:\n",
    "    - best_features: Optimal number of features for the highest F1 Macro score.\n",
    "    - best_score: The highest F1 Macro score achieved.\n",
    "    - scores_list: List of F1 Macro scores for each number of features.\n",
    "    \"\"\"\n",
    "    nof_list = np.arange(1, max_features + 1)\n",
    "    high_score = 0\n",
    "    best_features = 0\n",
    "    scores_list = []\n",
    "\n",
    "    for n in nof_list:\n",
    "        rfe = RFE(model, n_features_to_select=n)\n",
    "        \n",
    "        # Transform training and validation sets with RFE\n",
    "        X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "        X_val_rfe = rfe.transform(X_val)\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit(X_train_rfe, y_train)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_val_pred = model.predict(X_val_rfe)\n",
    "        \n",
    "        # Calculate F1 Macro score\n",
    "        score = f1_score(y_val, y_val_pred, average='macro')\n",
    "        scores_list.append(score)\n",
    "        \n",
    "        if score > high_score:\n",
    "            high_score = score\n",
    "            best_features = n\n",
    "    \n",
    "    print(f\"Optimum number of features: {best_features}\")\n",
    "    print(f\"F1 Macro Score with {best_features} features: {high_score:.6f}\")\n",
    "    \n",
    "    return best_features, high_score, scores_list\n",
    "\n",
    "#Function to plot decision tree feature importance\n",
    "def compare_feature_importances(X_train, y_train, figsize=(13, 5)):\n",
    "    \"\"\"\n",
    "    Compares feature importances using Gini and Entropy criteria in a Decision Tree Classifier \n",
    "    and visualizes the results in a bar plot.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training feature set (DataFrame or array with column names).\n",
    "    - y_train: Target labels for training.\n",
    "    - figsize: Tuple specifying the figure size for the plot (default=(13, 5)).\n",
    "\n",
    "    Returns:\n",
    "    - zippy: DataFrame containing feature importances for Gini and Entropy.\n",
    "    \"\"\"\n",
    "    # Calculate feature importances using Gini and Entropy criteria\n",
    "    gini_importance = DecisionTreeClassifier().fit(X_train, y_train).feature_importances_\n",
    "    entropy_importance = DecisionTreeClassifier(criterion='entropy').fit(X_train, y_train).feature_importances_\n",
    "    \n",
    "    # Create a DataFrame to store and organize the feature importances\n",
    "    zippy = pd.DataFrame(zip(gini_importance, entropy_importance), columns=['gini', 'entropy'])\n",
    "    zippy['col'] = X_train.columns  # Add column names\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting with Seaborn\n",
    "    tidy = zippy.melt(id_vars='col').rename(columns=str.title)\n",
    "    tidy.sort_values(['Value'], ascending=False, inplace=True)\n",
    "    \n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(y='Col', x='Value', hue='Variable', data=tidy)\n",
    "    plt.title(\"Feature Importances: Gini vs Entropy\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.legend(title=\"Criterion\")\n",
    "    plt.show()\n",
    "    \n",
    "    return zippy\n",
    "\n",
    "#This function is it to compare feature importances using Gini and Entropy criteria in a Random Forest Classifier and visualizes the results in a bar plot.\n",
    "def compare_rf_feature_importances(X_train, y_train, figsize=(13, 5), random_state=42):\n",
    "    \"\"\"\n",
    "    Compares feature importances using Gini and Entropy criteria in a Random Forest Classifier \n",
    "    and visualizes the results in a bar plot.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training feature set (DataFrame or array with column names).\n",
    "    - y_train: Target labels for training.\n",
    "    - figsize: Tuple specifying the figure size for the plot (default=(13, 5)).\n",
    "    - random_state: Random state for reproducibility (default=42).\n",
    "\n",
    "    Returns:\n",
    "    - importances: DataFrame containing feature importances for Gini and Entropy.\n",
    "    \"\"\"\n",
    "    # Calculate feature importances using Gini and Entropy criteria\n",
    "    gini_importance = RandomForestClassifier(random_state=random_state).fit(X_train, y_train).feature_importances_\n",
    "    entropy_importance = RandomForestClassifier(criterion='entropy', random_state=random_state).fit(X_train, y_train).feature_importances_\n",
    "    \n",
    "    # Create a DataFrame to store and organize the feature importances\n",
    "    importances = pd.DataFrame({\n",
    "        'gini': gini_importance,\n",
    "        'entropy': entropy_importance,\n",
    "        'col': X_train.columns\n",
    "    })\n",
    "    \n",
    "    # Melt the DataFrame for easier plotting with Seaborn\n",
    "    tidy = importances.melt(id_vars='col').rename(columns=str.title)\n",
    "    tidy.sort_values(['Value'], ascending=False, inplace=True)\n",
    "    \n",
    "    # Plot the feature importances\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(y='Col', x='Value', hue='Variable', data=tidy)\n",
    "    plt.title(\"Random Forest Feature Importances: Gini vs Entropy\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.legend(title=\"Criterion\")\n",
    "    plt.show()\n",
    "    \n",
    "    return importances\n",
    "\n",
    "\n",
    "#This function is used to select top k features based on scores using Chi-square test.\n",
    "def select_high_score_features_chi2_no_model(X_train, y_train, threshold=25):\n",
    "    \"\"\"\n",
    "    Performs Chi-square test to select top k features based on scores.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training feature set (DataFrame).\n",
    "    - y_train: Training target labels.\n",
    "    - threshold: Number of top features to select (default=25).\n",
    "\n",
    "    Returns:\n",
    "    - high_score_features: List of top feature names based on Chi-square scores.\n",
    "    - scores: Corresponding Chi-square scores of the selected features.\n",
    "    \"\"\"\n",
    "    # Perform Chi-square test\n",
    "    feature_scores = SelectKBest(chi2, k=threshold).fit(X_train, y_train).scores_\n",
    "\n",
    "    # Select top features\n",
    "    high_score_features = []\n",
    "    scores = []\n",
    "    for score, f_name in sorted(zip(feature_scores, X_train.columns), reverse=True)[:threshold]:\n",
    "        high_score_features.append(f_name)\n",
    "        scores.append(score)\n",
    "\n",
    "    print(f\"Top {threshold} features based on Chi-square scores:\", high_score_features)\n",
    "    print(\"Corresponding Chi-square scores:\", scores)\n",
    "\n",
    "    return high_score_features, scores\n",
    "\n",
    "#This function is used to select top k features based on scores using MIC test.\n",
    "def select_high_score_features_MIC(X_train, y_train, threshold=25, random_state=42):\n",
    "    \"\"\"\n",
    "    Selects the top features based on Mutual Information Criterion (MIC).\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training feature set (DataFrame or array).\n",
    "    - y_train: Training target labels (array-like).\n",
    "    - threshold: Number of top features to select (default=25).\n",
    "    - random_state: Random state for reproducibility (default=42).\n",
    "\n",
    "    Returns:\n",
    "    - high_score_features: List of top feature names based on MIC scores.\n",
    "    - scores: Corresponding MIC scores of the selected features.\n",
    "    \"\"\"\n",
    "    # Calculate MIC scores\n",
    "    feature_scores = mutual_info_classif(X_train, y_train, random_state=random_state)\n",
    "\n",
    "    # Select top features based on MIC scores\n",
    "    high_score_features = []\n",
    "    scores = []\n",
    "    for score, f_name in sorted(zip(feature_scores, X_train.columns), reverse=True)[:threshold]:\n",
    "        high_score_features.append(f_name)\n",
    "        scores.append(score)\n",
    "\n",
    "    print(f\"Top {threshold} features based on MIC scores:\", high_score_features)\n",
    "    print(\"Corresponding MIC scores:\", scores)\n",
    "\n",
    "    return high_score_features, scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbf60b2",
   "metadata": {},
   "source": [
    "### **Categorical Feature Selection Results**\n",
    "\n",
    "The following table summarizes the decisions for each categorical feature based on **Mutual Information (MIC)** and **Chi-Squared** (X²) results. The retained features will be used in subsequent modeling to enhance predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1169cd1",
   "metadata": {},
   "source": [
    "# Modeling <a class=\"anchor\" id=\"chapter5\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b8878",
   "metadata": {},
   "source": [
    "Logistic Rrgression\n",
    "\n",
    "KNN\n",
    "\n",
    "RandomForest\n",
    "\n",
    "Grid Search\n",
    "\n",
    "Gradient Boosting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1fdd8d",
   "metadata": {},
   "source": [
    "____"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
