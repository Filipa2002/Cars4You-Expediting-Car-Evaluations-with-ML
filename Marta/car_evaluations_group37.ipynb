{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    .header-banner {\n",
    "        background-color: white;\n",
    "        color: black; \n",
    "        padding: 1rem; \n",
    "        font-family: 'Nunito', sans-serif;\n",
    "    }\n",
    "    .header-content {\n",
    "        max-width: 2000px;\n",
    "        margin: 0 auto;\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        gap: 2rem;\n",
    "    }\n",
    "    .logo {\n",
    "        max-width: 160px;\n",
    "    }\n",
    "    .text-content {\n",
    "        flex: 1;\n",
    "    }\n",
    "    .text-content h1 {\n",
    "        font-size: 34px;\n",
    "        margin: 0 0 10px;\n",
    "        font-weight: 700;\n",
    "        color: #7e4d02ff;\n",
    "        border-bottom: 2px solid #e5c120ff;\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "    .text-content h2 {\n",
    "        font-size: 21px;\n",
    "        margin: 0 0 5px;\n",
    "        font-weight: 600;\n",
    "        color: #222;\n",
    "    }\n",
    "    .member-list {\n",
    "        display: grid;\n",
    "        grid-template-columns: repeat(2, auto);\n",
    "        gap: 6px 40px;\n",
    "        font-size: 17px;\n",
    "        color: #444;\n",
    "    }\n",
    "    .member {\n",
    "        position: relative;\n",
    "        padding-left: 20px;\n",
    "    }\n",
    "</style>\n",
    "<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
    "<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Nunito:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<header class=\"header-banner\">\n",
    "    <div class=\"header-content\">\n",
    "        <img src=\"https://i.ibb.co/JBPWVYR/Logo-Nova-IMS-Black.png\" alt=\"NOVA IMS Logo\" class=\"logo\">\n",
    "        <div class=\"text-content\">\n",
    "            <h1>Cars 4 You: Expediting Car Evaluations with ML</h1>\n",
    "            <h2>Group 37</h2>\n",
    "            <div class=\"member-list\">\n",
    "                <div class=\"member\">Filipa Pereira, 20240509</div>\n",
    "                <div class=\"member\">Gonçalo Silva, 20250354</div>\n",
    "                <div class=\"member\">Marta La Feria, 20211051 </div>\n",
    "                <div class=\"member\">Tomás Coroa, 20250394 </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</header>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f8f4ef; border-left:5px solid #d4b781ff; padding:12px 18px; border-radius:8px; font-family:sans-serif; font-size:14px; line-height:1.4; width:fit-content;\">\n",
    "\n",
    "<b style=\"color:#4b2e05;\"> Index</b>\n",
    "\n",
    "<ol style=\"margin:8px 0 0 18px; padding:0;\">\n",
    "\n",
    "\n",
    "  <li><a href=\"#sec1-identifying-business-needs\" style=\"color:#4b2e05; text-decoration:none;\">Identifying Business Needs</a></li>\n",
    "  <li><a href=\"#sec2-data-exploration-and-preprocessing\" style=\"color:#4b2e05; text-decoration:none;\">Data Exploration and Preprocessing</a>\n",
    "    <ul style=\"margin:4px 0 4px 15px; list-style-type:circle;\">\n",
    "      <li><a href=\"#sec21-data-content\" style=\"color:#4b2e05;\">2.1. Data Content</a></li>\n",
    "      <li><a href=\"#sec22-descriptive-statistics\" style=\"color:#4b2e05;\">2.2. Descriptive Statistics</a></li>\n",
    "      <li><a href=\"#sec23-explore-data-visualizations\" style=\"color:#4b2e05;\">2.3. Explore Data Visualizations</a></li>\n",
    "      <li><a href=\"#sec24-inconsistency-checks\" style=\"color:#4b2e05;\">2.4. Inconsistency Checks</a>\n",
    "        <ul style=\"margin:3px 0 3px 15px; list-style-type:square;\">\n",
    "          <li><a href=\"#sec241-brand-model\" style=\"color:#4b2e05;\">2.4.1. Brand & Model</a></li>\n",
    "          <li><a href=\"#sec242-year\" style=\"color:#4b2e05;\">2.4.2. Year</a></li>\n",
    "          <li><a href=\"#sec243-transmission\" style=\"color:#4b2e05;\">2.4.3. Transmission</a></li>\n",
    "          <li><a href=\"#sec244-mileage\" style=\"color:#4b2e05;\">2.4.4. Mileage</a></li>\n",
    "          <li><a href=\"#sec245-fueltype\" style=\"color:#4b2e05;\">2.4.5. Fuel Type</a></li>\n",
    "          <li><a href=\"#sec246-tax\" style=\"color:#4b2e05;\">2.4.6. Tax</a></li>\n",
    "          <li><a href=\"#sec247-mpg\" style=\"color:#4b2e05;\">2.4.7. MPG</a></li>\n",
    "          <li><a href=\"#sec248-enginesize\" style=\"color:#4b2e05;\">2.4.8. Engine Size</a></li>\n",
    "          <li><a href=\"#sec249-paintquality\" style=\"color:#4b2e05;\">2.4.9. Paint Quality</a></li>\n",
    "          <li><a href=\"#sec2410-previous-owners\" style=\"color:#4b2e05;\">2.4.10. Previous Owners</a></li>\n",
    "          <li><a href=\"#sec2411-hasdamage\" style=\"color:#4b2e05;\">2.4.11. Has Damage</a></li>\n",
    "        </ul>\n",
    "      </li>\n",
    "      <li><a href=\"#sec25-missing-data\" style=\"color:#4b2e05;\">2.5. Missing Data</a></li>\n",
    "      <li><a href=\"#sec26-explore-data-visualizations-continued\" style=\"color:#4b2e05;\">2.6. Explore Data Visualizations (Continued)</a></li>\n",
    "      <li><a href=\"#sec27-outliers\" style=\"color:#4b2e05;\">2.7. Outliers</a></li>\n",
    "      <li><a href=\"#sec28-create-extra-features\" style=\"color:#4b2e05;\">2.8. Create Extra Features</a></li>\n",
    "      <li><a href=\"#sec29-categorical-variables\" style=\"color:#4b2e05;\">2.9. Encoding Categorical Variables</a></li>\n",
    "      <li><a href=\"#sec210-data-scaling\" style=\"color:#4b2e05;\">2.10. Data Scaling</a></li>\n",
    "      <li><a href=\"#sec211-feature-selection\" style=\"color:#4b2e05;\">2.11. Feature Selection</a></li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li><a href=\"#sec3-regression-benchmarking\" style=\"color:#4b2e05;\">Regression Benchmarking</a></li>\n",
    "</ol>\n",
    "<hr style=\"border: 0; height: 1px; background-color: rgba(75,46,5,0.3); margin: 15px 0;\">\n",
    "<p style=\"margin-top:6px; font-size:12px; color:#4b2e05;\">\n",
    "\n",
    "  <b>Note:</b> Sections *Feature Selection* and *Regression Benchmarking* are included in the second notebook.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abstract\n",
    "\n",
    "**Cars 4 You** is an online car resale company that buys vehicles from multiple brands and resells them for profit. The company ims to accelerate its car evaluation process by predicting purchase prices directly from user-provided data, reducing reliance on mechanic inspections. Using the provided 2020 dataset, we approach this as a supervised regression problem with mixed numerical and categorical variables.\n",
    "\n",
    "After an initial exploratory analysis to understand distributions, relationships, and data quality, we design a preprocessing pipeline that handles missing values, encodes categorical features, engineers meaningful variables such as car age and mileage per year, and applies scaling.\n",
    "\n",
    "<span style=\"color:red;\">(ESCREVER SOBRE MODEL ASSESMENT E FEACTURE SELECTION)</span>\n",
    "\n",
    "The final selected pipeline produces predictions for the independent test set, generating the Kaggle submission file (carID, price)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Group Member Contribution\n",
    "**Filipa Pereira** –  (40%)  \n",
    "**Gonçalo Silva** –   (20%)  \n",
    "**Marta La Feria** –  (20%)  \n",
    "**Tomás Coroa** – (20%)  \n",
    "_All members contributed to discussions, review and documentation._\n",
    "\n",
    "Data exploration, preprocessing design, visual analytics,Modeling, feature selection, cross-validation framework, Optimization, open-ended study, and deployment packaging ...\n",
    "\n",
    "<span style=\"color:red;\">completar</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec1-identifying-business-needs\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px rgba(227, 167, 108, 1);\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: #644712ff;\"><b>1 | Identifying Business Needs</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview and main goals.**  \n",
    "The goal of this project is to develop a simple and reliable model to predict car prices based on user-provided features, reducing the need for mechanic inspections and speeding up Cars 4 You’s evaluation process. The work focuses on data exploration, preprocessing, and building an interpretable regression model that can generalize well to unseen data.  \n",
    "\n",
    "**Model assessment approach.**  \n",
    "For this first submission, we used the **Holdout method** as it is simple and allows for a quick evaluation of the models’ performance. This choice is supported by the fact that we have a large dataset with similar distributions between the training and test sets, as will be shown later. However, we acknowledge that this approach may introduce some variability, therefore, in the second submission, we plan to apply *k-Fold Cross-Validation*(subject to computational constraints) as it is a more robust and reliable method for estimating the model’s generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: center; margin-top: 20px;\">\n",
    "\n",
    "\n",
    "|       | **Attribute**                               | **Description**                                                                  |\n",
    "|-------|--------------------------------------------|---------------------------------------------------------------------------------|\n",
    "| **0** | `carID`                               | An attribute that contains an identifier for each car                                                     |\n",
    "| **1** | `Brand`                                 | The car’s main brand (e.g. Ford, Toyota)                                                               |\n",
    "| **2** | `model`                              | The car model                                  |\n",
    "| **3** | `year` | The year of Registration of the Car                   |\n",
    "| **4** | `mileage`               |     The total reported distance travelled by the car (in miles)               |  \n",
    "| **5** | `tax`                        | The amount of road tax (in £) that, in 2020, was applicable to the car in question                            |\n",
    "| **6** | `fuelType`                         | Type of Fuel used by the car (Diesel, Petrol, Hybrid, Electric)                             |\n",
    "| **7** | `mpg`                                   | Average Miles per Gallon                                      |\n",
    "| **8** | `engineSize`                                    | Size of Engine in liters (Cubic Decimeters)                                       |\n",
    "| **9** | `paintQuality%`                                    | The mechanic’s assessment of the cars’ overall paint quality and hull integrity (filled by the mechanic during evaluation)                                       |\n",
    "| **10**| `previousOwners`                                | Number of previous registered owners of the vehicle                                   |\n",
    "| **11**| `hasDamage`     | Boolean marker filled by the seller at the time of registration stating whether the car is damaged or not                 |\n",
    "| **12**| `price`                        | The car’s price when purchased by Cars 4 You (in £)                           |\n",
    "\n",
    "\n",
    "</div  >\n",
    "\n",
    " <b>Note:</b> The variable `transmission` is not presented in the metadata but we will consider it as type of transmission of the car (Automatic, Manual, Semi-Automatic, Other).\n",
    "  <div style=\"background-color:#e5c120ff; padding:1px; border-radius:10px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import liberies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cars_api_request import run_cars_api_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\anaconda3\\envs\\Fall2526\\Lib\\site-packages\\requests\\models.py:976\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    978\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    979\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\anaconda3\\envs\\Fall2526\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\anaconda3\\envs\\Fall2526\\Lib\\json\\decoder.py:337\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    334\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    335\u001b[39m \n\u001b[32m    336\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\anaconda3\\envs\\Fall2526\\Lib\\json\\decoder.py:355\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcars_api_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m\\\\wsl.localhost\\Ubuntu\\home\\mpais-go\\ML_project\\Marta\\cars_api_request.py:14\u001b[39m, in \u001b[36mcars_api_request\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m         \u001b[38;5;28mnext\u001b[39m(f)\n\u001b[32m     12\u001b[39m         done = {line.split(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f}\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m makes = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBASE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/makes/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mAPI_KEY\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m makes = makes[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(makes, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m makes \u001b[38;5;28;01melse\u001b[39;00m makes\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUT, \u001b[33m\"\u001b[39m\u001b[33ma\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Utilizador\\anaconda3\\envs\\Fall2526\\Lib\\site-packages\\requests\\models.py:980\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    978\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    979\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "run_cars_api_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### APAGAR ### NO FIM GARANTIR QUE SÓ TEMOS OS IMPORTS NECESSÁRIOS E NÃO EXISTEM IMPORT NOUTROS SÍTIOS\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For the split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import ceil\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# To deal with inconsistencies\n",
    "from rapidfuzz import process\n",
    "from datetime import datetime\n",
    "import unicodedata, re, difflib\n",
    "from difflib import SequenceMatcher\n",
    "from thefuzz import fuzz\n",
    "from typing import Optional\n",
    "import random\n",
    "\n",
    "# To deal with missing values\n",
    "from scipy.stats import ttest_ind, chi2_contingency # For statistical tests\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# For encoding categorical variables\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# For scaling\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
    "\n",
    "# Disable FutureWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Set the style of the visualization\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x) # display floats with 2 decimal places\n",
    "\n",
    "# for better resolution plots\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Setting seaborn style\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style='white')\n",
    "\n",
    "#For reproducibility\n",
    "random.seed(37) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both training and testing datasets\n",
    "car_eval = pd.read_csv('../project_data/train.csv')\n",
    "X_test = pd.read_csv('../project_data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#e4b3c2ff; border:1.5px solid #e4b3c2ff; border-radius:8px; padding: 10px; width:1130px; text-align: justify;\">\n",
    "### APAGAR ### <br>\n",
    "MARTA EXPLICA\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_model_dic = pd.read_csv('../project_data/brands_models.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec2-data-exploration-and-preprocessing\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px rgba(227, 167, 108, 1);\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: #644712ff;\"><b>2 | Data Exploration and Preprocessing</b></span>\n",
    "    </h1>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color:#e4b3c2ff; border:1.5px solid #e4b3c2ff; border-radius:8px; padding: 10px; width:1130px; text-align: justify;\">\n",
    "### APAGAR ### <br>\n",
    "\n",
    "- Description of data received -> key insights ✅\n",
    " - Steps taken to clean and prepare the data based on exploration\n",
    "<br><br>\n",
    " - Check data contents, provide descriptive statistics and check for inconsistencies in the data.✅\n",
    " - Explore data visually and extract relevant insights. Explain your rationale and findings.✅ Do not forget to analyse **multivariate relationships**.\n",
    " - Are there any missing values? Take action to handle them.\n",
    " - Check the dataset for outliers and pre-process them. Justify your decisions.\n",
    " - Deal with categorical variables.\n",
    " - Review current features and create extra features if needed. Explain your steps.\n",
    " - Perform data scaling. Explain the reasoning behind your choices.\n",
    " - Define and implement a clear and unambiguous strategy for **feature selection**. Use the methods discussed in the course. Present and justify your final selection.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec21-data-content\" \n",
    "     style=\"background-color:#644712c5; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px #644712c5;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(242, 226, 9, 1);\"><b>2.1 | Data Content</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> By observing just a few rows, we can already identify both categorical and numerical variables. Since categorical variables are present, we know that some type of encoding will be required before using them in the model. Some of the numerical variables (like `year` and `previousOwners`) should not be treated as continuous . It is also noticeable that there are missing values, at least in the variable `tax`. Additionally, there are some spelling errors and inconsistencies in text formatting, such as differences in lowercase and uppercase (ex: `transmission` includes both \"Manual\" and \"anual\").\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_eval.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The train and test datasets contain the same variables, except for the target variable `price`, as expected. The data types are consistent across both datasets, however, some issues are present. <BR><BR>\n",
    "The training set contains 75,973 records and all variables have missing values except for the identifier `carID` and the target variable `price`.\n",
    "The test set contains 32,567 records and also shows missing values in all variables except for the identifier `carID`. <BR><BR>\n",
    "Moreover, it can be noted that the variables `year` and `previousOwners` should indeed be numeric, but of integer type (int64) rather than continuous (float64). The variable `hasDamage` should be boolean, as a car either has damage or it does not. Finally, `carID` should be categorical, but since it is a unique identifier and will not be used in the model, there is no need to modify its data type.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates\n",
    "print(f\"Total duplicates: {car_eval.duplicated().sum()}\")\n",
    "#Check for duplicates without the carID column\n",
    "print(f\"Duplicates without carID: {car_eval.drop(columns=['carID']).duplicated().sum()}\")\n",
    "#Check for duplicates without the price column\n",
    "print(f\"Duplicates without price and carID: {car_eval.drop(columns=['price', 'carID']).duplicated().sum()}\")\n",
    "\n",
    "# Repeat for the test set\n",
    "print(f\"Total duplicates in test set: {X_test.duplicated().sum()}\")\n",
    "print(f\"Duplicates without carID in test set: {X_test.drop(columns=['carID']).duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that although there are no duplicates when considering all columns, there are 4 duplicates in the training set when the `carID` column is excluded. We consider these true duplicates and therefore we will remove them (two different cars are unlikely to have exactly the same features). <br><br>\n",
    "Additionally, there are more 7 duplicates when excluding both `carID` and `price`. Since `carID` is only an identifier and not an input variable, having identical inputs with different target values would confuse the model; there would be no apparent reason for predicting different prices. This increases noise and reduces the model’s generalization ability, so we will keep only the most recent submission for each car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates without considering price and carID, keeping the most recent entry\n",
    "car_eval = car_eval.sort_values('carID').drop_duplicates(subset=car_eval.columns.difference(['carID', 'price']).tolist(), keep='last')\n",
    "\n",
    "print(f\"Duplicates without carID: {car_eval.drop(columns=['carID']).duplicated().sum()}\")\n",
    "print(f\"Duplicates without price and carID: {car_eval.drop(columns=['price', 'carID']).duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set \"carID\" as index since it is a unique identifier as confirmed by the .info() output\n",
    "car_eval.set_index('carID', inplace=True)\n",
    "X_test.set_index('carID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec22-descriptive-statistics\" \n",
    "     style=\"background-color:#644712c5; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px #644712c5;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(242, 226, 9, 1);\"><b>2.2 | Descriptive Statistics</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical columns\n",
    "numeric_cols = car_eval.select_dtypes(include='number')\n",
    "\n",
    "# Describe\n",
    "numeric_desc = numeric_cols.describe().T\n",
    "\n",
    "# Add skewness and kurtosis\n",
    "numeric_desc['skew'] = numeric_cols.skew()\n",
    "numeric_desc['kurtosis'] = numeric_cols.kurtosis()\n",
    "numeric_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the insights drawn so far from the numerical variables in the training dataset, we can further highlight:\n",
    "- **`year`**: has a distribution centered around 2017, indicating that most cars in the dataset are relatively recent (from 2016 onwards). The minimum value of 1970 reveals a few much older vehicles. The negative skewness (-1.85) confirms a concentration of newer models, with a long left tail representing these rare older cars. The high kurtosis (11.8) further suggests a narrow and peaked distribution with extreme values, reinforcing that while some cars are old, the vast majority belong to modern years.\n",
    "\n",
    " - **`price`**: has a mean of around 16,882 with considerable dispersion (std ≈ 9,737). About 75% of cars cost up to roughly 20,950, while the maximum price reaches nearly 160,000, indicating the presence of some luxury vehicles. The positive skew (2.28) shows a strongly right-skewed distribution, with most cars priced lower but a few extremely high values. The high kurtosis (12.0) further highlights a long-tailed distribution with notable outliers at the upper end.\n",
    "\n",
    "- **`mileage`**: shows an average of around 23,000 miles with considerable dispersion (standard deviation ≈ 22,130), reflecting the diversity in vehicle usage. The quartiles indicate that 50% of cars have between approximately 7,400 and 32,400 miles, typical of relatively recent vehicles with regular use. The positive skewness (1.55) shows that most vehicles have low to moderate mileage, while a few have very high values. The kurtosis (5.23) confirms the presence of some outliers. The negative minimum value (-58,540) is unrealistic and will be corrected, although it does not substantially alter the overall pattern of the variable.\n",
    "<!-- affecting the mean, standard deviation, and the lower tail. Despite this, the positive skew (1.56) indicates that most cars have moderate mileage, with a few high-mileage outliers -->\n",
    "\n",
    "- **`tax`**: has an average of around £120, with values mostly concentrated between £125 and £145 (25th and 75th percentiles), suggesting that most cars fall within similar tax bands, typical of low- to mid-emission vehicles. The dispersion is moderate (std ≈ 66), and the near-zero skewness (0.05) indicates an approximately symmetric distribution, further supported by the kurtosis (3.56) close to normality. The negative value (-91.12) is logically impossible and slightly affects the symmetry of the distribution. We will treat or remove it to prevent bias in further analyses and predictive models.\n",
    "\n",
    "- **`mpg`**: has an average of about 55 mpg, with most values ranging between 46 and 63 mpg, reflecting a dataset dominated by vehicles with fuel efficiency typical of modern cars. However, the strong right skewness (7.30) and extremely high kurtosis (158.66) indicate the presence of severe outliers that should be carefully examined. The negative minimum (-43.42) is physically impossible and should be removed. If not properly treated, this variable could distort its relationship with `price` in the final model and harm predictive performance.\n",
    "\n",
    "- **`engineSize`**: presents a realistic distribution, with an average of 1.66L and most vehicles between 1.2L and 2.0L. The positive skewness (1.23) and slightly high kurtosis (4.44) indicate a few cars with larger engines, up to 6.6L. The negative minimum value (-0.1)  is technically impossible and may slightly influence the positive skew.\n",
    "\n",
    "- **`paintQuality%`**: presents a balanced distribution (skew ≈ 0, kurtosis ≈ -0.8), suggesting values are spread almost symmetrically around the mean (64.6). Most cars have paint quality between 47% and 82%, with few extreme values. This variable seems approximately normal, with moderate dispersion (std = 21.0). The maximum value (125.59%) is unrealistic for a percentage scale.\n",
    "\n",
    "- **`previousOwners`**: displays a symmetric distribution (skew ≈ 0), centered around 2 previous owners on average, with most cars having between 1 and 3 owners. Kurtosis is slightly negative (-0.85), indicating a flatter-than-normal shape (less concentration around the mean). The negative minimum value (-2.35) is clearly invalid, and after removing these negative entries, the distribution is expected to approximate a normal shape.\n",
    "\n",
    "- **`hasDamage`**: this analysis is not strictly appropriate, since the variable is binary (though its type has not yet been converted). Nonetheless, we can see no variation, all values are 0. This makes the variable meaningless in its current form, as it should be boolean (0 = no damage, 1 = damaged). Since the “1” category never appears, the feature provides no analytical value. It could be removed from the model, although we will further investigate its origin.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical columns\n",
    "numeric_cols_t = X_test.select_dtypes(include='number')\n",
    "\n",
    "# Describe\n",
    "numeric_desc_t = numeric_cols_t.describe().T\n",
    "\n",
    "# Add skewness and kurtosis\n",
    "numeric_desc_t['skew'] = numeric_cols_t.skew()\n",
    "numeric_desc_t['kurtosis'] = numeric_cols_t.kurtosis()\n",
    "numeric_desc_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the test set in comparison with the previously analyzed training set:\n",
    "- **`year`**: maintains almost the same distribution, with an identical mean and equal interquartile ranges, but with a higher minimum value (1991 vs. 1970), indicating the absence of the oldest vehicles in the test set. The lower skewness (-1.61 vs. -1.85) and kurtosis (6.81 vs. 11.8) suggest a slightly less concentrated distribution with fewer extreme values.\n",
    "\n",
    "- **`mileage`**: shows values that are nearly identical, including the same impossible negative minimum, showing that the same type of measurement error persists. However, there is a slight reduction in skewness (1.49 vs. 1.55) and kurtosis (4.46 vs. 5.23), suggesting a somewhat more balanced distribution.\n",
    "\n",
    "- **`tax`**: exhibits an almost identical distribution between the two datasets, again including the invalid negative value. Both the mean and quartiles are nearly the same, indicating structural consistency in the test set. Minor variations in skewness (0.08 vs. 0.05) and kurtosis (3.74 vs. 3.56) are not significant.\n",
    "\n",
    "- **`mpg`**: although the central statistics remain identical, the test set shows greater dispersion (std 17.64 vs. 16.50), higher skewness (8.76 vs. 7.30), and even higher kurtosis (186.44 vs. 158.66), reflecting a stronger presence of extreme values. The negative values remain and must be corrected, as they significantly influence the distribution and may harm the performance of the price prediction model.\n",
    "\n",
    "- **`engineSize`**: behaves almost identically to the training set, with similar mean and dispersion. The slight decrease in skewness (1.18 vs. 1.23) and kurtosis (3.93 vs. 4.44) indicates a marginally more homogeneous distribution in the test set.\n",
    "\n",
    "- **`paintQuality%`**: shows very similar distributions across both datasets. The means (≈64.5) and quartiles remain practically unchanged, as do the shape measures (skewness and kurtosis near zero), demonstrating stability and good representativeness of this variable between training and testing.\n",
    "\n",
    "- **`previousOwners`**: has nearly identical values, including the same impossible negative minimum (-2.35). The means (≈2) and quartiles match, and the distribution remains symmetric, confirming consistency across datasets, although the negative anomaly still needs correction. \n",
    "\n",
    "- **`hasDamage`**: continues to show no variation, containing only zeros in both datasets. The recommendation stands where its source should be reviewed, and its removal from the model should be considered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical columns\n",
    "cat_cols = car_eval.select_dtypes(include='object')\n",
    "\n",
    "# Basic describe for categorical variables\n",
    "cat_desc = cat_cols.describe().T\n",
    "\n",
    "# Add proportion of the most frequent category\n",
    "cat_desc['top_freq_ratio'] = cat_desc['freq'] / cat_desc['count']\n",
    "\n",
    "cat_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The categorical attributes in the training dataset also provide important insights:\n",
    "\n",
    "- **`Brand`**: contains 72 distinct brands. The most frequent brand is `Ford`, representing 20%.\n",
    "\n",
    "- **`model`**: shows a high level of variability (735 unique models). The most common model is the `Focus` (primarily associated with the Ford brand), accounting for 9% of all observations.\n",
    "\n",
    "- **`transmission`**: includes 40 distinct categories, though the variable is clearly dominated by `Manual` vehicles, which make up about 51.1% of the dataset. This reflects typical market patterns, where manual transmission vehicles are still prevalent, particularly among mid-range and older models.\n",
    "\n",
    "- **`fuelType`**: consists of 34 distinct fuel categories, with `Petrol` as the dominant type (≈51.0% of observations). This prevalence aligns with general automotive market trends, though the inclusion of other fuel types introduces potentially valuable variation for analysing price effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical columns\n",
    "cat_cols_t = X_test.select_dtypes(include='object')\n",
    "\n",
    "# Basic describe for categorical variables\n",
    "cat_desc_t = cat_cols_t.describe().T\n",
    "\n",
    "# Add proportion of the most frequent category\n",
    "cat_desc_t['top_freq_ratio'] = cat_desc_t['freq'] / cat_desc_t['count']\n",
    "\n",
    "cat_desc_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the categorical variables in the test set in comparison with the previously analyzed training set:\n",
    "\n",
    "- **`Brand`**: contains slightly fewer unique brands (64 vs. 72). However, `Ford` continues to dominate, representing 20% of the observations in both datasets.\n",
    "\n",
    "- **`model`**: there is a noticeable reduction in the number of unique models (593 vs. 735) probably due to the smaller sample size of the test set. Despite this, `Focus` remains the most frequent model, accounting for 9% of all records.\n",
    "\n",
    "- **`transmission`**: the most common transmission type is still `Manual`, appearing in just over half of the entries (≈51%). The number of unique categories is also very similar (38 vs. 40), implying that the test set preserves the main diversity of transmission types found in the training data.\n",
    "\n",
    "- **`fuelType`**: `Petrol` remains the dominant fuel type (50% vs. 51%). The slight decrease in the number of unique fuel types (29 vs. 34) likely reflects the absence of some rare or less common categories in the test sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that, as we have already observed and will further explore later, there are numerous spelling errors and text formatting inconsistencies within the categorical variables, which significantly affect the current analysis and our overall understanding of the data. The present examination, regarding the categorical variables,mainly serves as a reference point for comparison with the results obtained after addressing these inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec23-explore-data-visualizations\" \n",
    "     style=\"background-color:#644712c5; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px #644712c5;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(242, 226, 9, 1);\"><b>2.3 | Explore Data Visualizations</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = car_eval.select_dtypes(include='number').columns\n",
    "cat = car_eval.select_dtypes(include='object').columns\n",
    "\n",
    "\n",
    "print(f'Numeric columns: {len(num)} {list(num)} \\n')\n",
    "print(f'Categorical columns: {len(cat)} {list(cat)}')\n",
    "\n",
    "palette = ['#5C4212','#a92f02', '#a55b1bf9', '#b08972', '#e3a76c', '#e5c120','#f39c06','#f2e209']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_hist(df, cols, title, n_bins=50): \n",
    "    \n",
    "    # Calculate number of subplot rows \n",
    "    sp_rows = ceil(len(cols) / 2)\n",
    "    \n",
    "    # Create figure with double rows: one for boxplots, one for histograms\n",
    "    fig, axes = plt.subplots(\n",
    "        sp_rows * 2, 2,\n",
    "        figsize=(14, sp_rows * 4.5),\n",
    "        gridspec_kw={'height_ratios': [0.3, 0.85] * sp_rows},  # boxplots shorter, histograms taller\n",
    "        tight_layout=False\n",
    "    )\n",
    "    \n",
    "    # Ensure axes is a 2D array for consistent indexing\n",
    "    axes = np.array(axes).reshape(sp_rows * 2, 2)\n",
    "\n",
    "    # Loop through each feature to plot\n",
    "    for idx, feat in enumerate(cols):\n",
    "        row, col = (idx // 2) * 2, idx % 2 # Determine that variable's plot position\n",
    "        ax_box, ax_hist = axes[row, col], axes[row + 1, col]\n",
    "\n",
    "        # Boxplot\n",
    "        sns.boxplot(x=df[feat], color=palette[-2], ax=ax_box, orient='h', width=0.58)\n",
    "        \n",
    "        # Remove axis labels\n",
    "        ax_box.set_xlabel(None)\n",
    "        ax_box.set_ylabel(None)\n",
    "        # Remove top and right borders\n",
    "        sns.despine(ax=ax_box, top=True, right=True)\n",
    "\n",
    "\n",
    "        # Histogram\n",
    "        sns.histplot(df[feat], bins=n_bins, color=palette[-2], kde=True, stat='percent', alpha=0.6, ax=ax_hist)\n",
    "            \n",
    "        # Calculate and plot statistics\n",
    "        stats = {\n",
    "            'mean': df[feat].mean(),\n",
    "            'median': df[feat].median(),\n",
    "            'q1': df[feat].quantile(0.25),\n",
    "            'q3': df[feat].quantile(0.75)\n",
    "        }\n",
    "        # Plot vertical lines for each statistic\n",
    "        for stat_name, stat_val in stats.items():\n",
    "            ax_hist.axvline(\n",
    "                stat_val, color=palette[list(stats.keys()).index(stat_name)], linestyle='--',\n",
    "                linewidth=1.5, alpha=0.8, label=f'{stat_name.capitalize()}: {stat_val:.1f}'\n",
    "            )\n",
    "        # Remove top and right borders\n",
    "        ax_hist.legend(loc='best', fontsize=7)\n",
    "        sns.despine(ax=ax_hist, top=True, right=True)\n",
    "\n",
    "    # Remove any unused subplots\n",
    "    for i in range(len(cols) * 2+1, sp_rows * 4):\n",
    "            fig.delaxes(axes.flatten()[i])\n",
    "        \n",
    "    # Title\n",
    "    plt.suptitle(\"Distribution of Numerical Car Features \" + title, fontweight='bold', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_hist(car_eval, num.drop(['hasDamage']), \"in the Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`Year`**: The distribution for year is left-skewed, characterized by a long tail towards older models, with the vast majority of vehicles clustered in the 2010-2020 range as noted previously. The outliers represent mainly the oldest vehicles, extending down towards 1970.\n",
    "\n",
    "- **`Price`**: The price distribution exhibits a notable positive skew, with a large number of vehicles concentrated at lower price points and a long tail extending significantly towards very high prices, which pulls the mean above the median. The visible outliers are these high-priced vehicles stretching past 40,000.\n",
    "\n",
    "- **`Mileage`**: As established, mileage shows a positive skew, concentrating most vehicles under 60,000 miles while having a long tail for higher mileages. Crucially, the outliers on the far left clearly show the presence of negative mileage values, confirming the existence of this error as noted before.\n",
    "\n",
    "- **`Tax`**: The tax variable displays a bimodal distribution. We observe the presence of invalid negative outliers, as noted earlier, as well as several high-end outliers. This indicates that while the bulk of the data is clustered within specific ranges, the distribution still extends toward the extreme upper end.\n",
    "\n",
    "- **`MPG`**: The Miles Per Gallon variable is characterized by a strong positive/right skew (mean > median),  with most of the data clustering in the lower-to-mid range MPG values and a long, thin tail extending towards higher MPG values, contributing to the presence of several right-side outliers. It also shows visible negative outliers, which are non-physical and will be addressed.\n",
    "\n",
    "- **`EngineSize`**: The engineSize plot presents a multimodal distribution, clearly showing distinct groups, likely corresponding to common engine displacements (e.g., peaks near 1, 1.5, and 2 liters). This variable shows a positive skew with some outliers on the higher end, as well as a negative outlier that is not physically possible.\n",
    "\n",
    "- **`PaintQuality%`**: The paintQuality% distribution is exceptionally stable and appears nearly uniform across the central range (approximately 30% to 90%). The shape measures being near zero (skew≈0 and kurtosis≈0) are visually evident in this flat distribution, and it does not show any outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_hist(X_test, num.drop(['hasDamage', 'price']), \"in the Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`Year`**: The distribution in the test set closely mirrors that of the training set, maintaining a left skew with most vehicles concentrated between 2010 and 2020. However, as previously noted, the left-side outliers start later, which results in a slight reduction in both kurtosis and skewness.\n",
    "\n",
    "- **`Mileage`**: It presents a distribution very similar to the training set: a positively skewed shape, with most values concentrated at lower mileage levels and a long tail extending toward higher mileages. The presence of negative values remains evident and must be addressed.\n",
    "\n",
    "- **`Tax`**: As in the training dataset, the tax distribution in the test set exhibits a somewhat bimodal pattern, with the presence of invalid negative outliers and several high-end outliers extending the upper tail.\n",
    "\n",
    "- **`MPG`**: It appears to have the exact same distribution as the training set, characterized by a strong positive skew. There are some impossible negative outliers, as well as some extreme values on the right tail, with one observation standing out markedly (exceeding 400 MPG).\n",
    "\n",
    "- **`EngineSize`**: It exhibits almost the same distribution as the training set: a positively skewed, multimodal pattern with distinct peaks around 1, 1.5, and 2 liters. Some high-end outliers are observed, along with one negative outlier that is clearly an error.\n",
    "\n",
    "- **`paintQuality%`**: The paintQuality% distribution in the test set also appears nearly uniform across the central range. Similarly, the visual flatness of the plot aligns with the near-zero skewness and kurtosis, with no apparent outliers present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is highly important that the variables in the training and test sets exhibit similar distributions, as is the case here, so that the model can perform reliably and generalize effectively to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_wordcloud(df, cols):\n",
    "    \n",
    "    # Loop through each column to create individual plots\n",
    "    for col in cols:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16,6), gridspec_kw={'width_ratios':[1,1]}) #1 row, 2 columns\n",
    "        \n",
    "        # Barplot\n",
    "        ax = axes[0]\n",
    "        \n",
    "        value_counts = df[col].value_counts(normalize=True) * 100 # percentage of each unique value in the column\n",
    "        # Up to top 10 unique values\n",
    "        if len(value_counts) > 10:\n",
    "            value_counts = value_counts.head(10)\n",
    "\n",
    "        sns.barplot(x=value_counts.values, y=value_counts.index, ax=ax, color='#a55b1bf9')\n",
    "\n",
    "        # Add percentage above each bar\n",
    "        for i, v in enumerate(value_counts.values):\n",
    "            ax.text(v + 0.5, i, f\"{v:.1f}%\", va='center', fontsize=9)\n",
    "        \n",
    "        ax.set_title(f\"{col} - Barplot\", fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(\"Percentage\")\n",
    "        ax.set_ylabel(None)\n",
    "        sns.despine(ax=ax, top=True, right=True)\n",
    "        \n",
    "\n",
    "        # WordCloud\n",
    "        ax_wc = axes[1]\n",
    "        word_counts = df[col].value_counts()\n",
    "        wc = WordCloud(width=800, height=400,\n",
    "                       background_color='white',\n",
    "                       colormap='YlOrBr',\n",
    "                       contour_width=1).generate_from_frequencies(word_counts)\n",
    "        ax_wc.imshow(wc, interpolation='bilinear')\n",
    "        ax_wc.axis('off')\n",
    "        ax_wc.set_title(f\"{col} - WordCloud\", fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_wordcloud(car_eval, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`Brand`**: the 3 brands with the highest presence in the training set are `Ford` (19.9%), `Mercedes` (14.4%), and `VW` – Volkswagen (13.1%). However, by examining the word cloud, we can see that there are many spelling errors and inconsistencies (e.g., `Opel`, `opel`, `pel`, `OPEL`). We were already aware that these issues existed, but this visualization provides a better understanding of the problem, as it could be affecting the order in which the brands appear. It will be more insightful to compare these results with those obtained after correcting inconsistencies across all variables.\n",
    "\n",
    "- **`model`**: The most common car models are `Focus` (8.5%) from Ford, `C Class` (6.6%) from Mercedes, and `Fiesta` (5.5%) from Ford, corresponding to the most frequent car brands. There are many different car models, which makes it difficult to determine whether the same inconsistencies observed in the brand variable are present here, though this possibility cannot be ruled out.\n",
    "\n",
    "- **`transmission`**: The most frequent transmission type is `Manual` (51.1%), followed by `Semi-Auto` (20.9%) and `Automatic` (18.8%). The word cloud reveals several spelling errors and inconsistencies, such as `anual`, `MANUAL`, and `manual`, which is affecting the statistics.\n",
    "\n",
    "- **`fuelType`**: The most common fuel type is `Petrol` (51.0%), followed by `Diesel` (38.2%), with a sharp drop to `Hybrid` (2.7%). The word cloud reveals several spelling errors and inconsistencies, such as `diesel`, `diese`, and `DIESEL`, which is affecting the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_wordcloud(X_test, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the variables in the test set exhibit the same distribution as in the training set, so we will skip this analysis to avoid being too repetitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison_barplots(dataframes_dict):\n",
    "    \n",
    "    n_dfs = len(dataframes_dict)\n",
    "    \n",
    "    if n_dfs == 0:\n",
    "        print(\"Error: No DataFrames provided for comparison.\")\n",
    "        return\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(1, n_dfs, figsize=(n_dfs * 4, 5), squeeze=False) \n",
    "\n",
    "    for i, (df_label, df) in enumerate(dataframes_dict.items()):\n",
    "        ax = axes[0, i] \n",
    "\n",
    "        # Replace NaN values with a string for visibility\n",
    "        df_temp = df['hasDamage'].fillna('Is Missing (NaN)')\n",
    "        \n",
    "        # Count all unique values\n",
    "        value_counts = df_temp.value_counts()\n",
    "\n",
    "        # Skip if no data to plot\n",
    "        if value_counts.empty:\n",
    "            ax.set_title(f\"hasDamage - {df_label} (No relevant data)\", fontsize=12, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        # Calculate total for percentage display\n",
    "        total_plotted = value_counts.sum()\n",
    "\n",
    "        # Vertical barplot\n",
    "        sns.barplot(x=value_counts.index, y=value_counts.values, ax=ax, color='#a55b1bf9', width=0.5)\n",
    "\n",
    "        # Add percentage labels\n",
    "        for j, v in enumerate(value_counts.values):\n",
    "            ax.text(j, v + (value_counts.max() * 0.02), \n",
    "                    f\"{v/total_plotted*100:.1f}%\", \n",
    "                    ha='center', va='bottom', fontsize=9, rotation=0)\n",
    "\n",
    "        # Titles and axis labels\n",
    "        ax.set_title(f\"hasDamage - {df_label}\", fontsize=12, fontweight='bold')\n",
    "        ax.set_ylabel(\"Count\") \n",
    "        sns.despine(ax=ax, top=True, right=True)\n",
    "\n",
    "    # Overall figure title\n",
    "    fig.suptitle(f\"Count Comparison for hasDamage Across Datasets\", fontweight='bold', fontsize=14, y=1.02)\n",
    "    \n",
    "    # Adjust layout spacing\n",
    "    plt.subplots_adjust(\n",
    "        left=0.05,        \n",
    "        right=0.98,         \n",
    "        wspace=0.3 if n_dfs > 1 else 0, \n",
    "        top=0.88            \n",
    "    )\n",
    "    \n",
    "    # Display plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'Train Set': car_eval,\n",
    "      'Test Set': X_test}\n",
    "\n",
    "plot_comparison_barplots(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the train and test sets show the same distribution. The percentage of missing values is small and may correspond to vehicles with damage. However, it seems odd that the mechanic would forget to fill this in. It would make more sense if it were missing for vehicles without damage. Therefore, this variable still requires further evaluation to determine how to handle the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filtered = [col for col in num if col not in ['previousOwners', 'hasDamage']]\n",
    "\n",
    "# Pairwise Relationship of Numerical Variables\n",
    "sns.pairplot(\n",
    "    car_eval[num_filtered],\n",
    "    diag_kind='hist',\n",
    "    plot_kws={'color': palette[0], 'alpha': 0.6},\n",
    "    diag_kws={'color': palette[-2]}\n",
    ")\n",
    "plt.suptitle('Pairwise Relationship of Numerical Variables in Train Set', fontweight='bold', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will postpone the multivariate analysis involving categorical variables until after addressing the existing inconsistencies. For now, we focus on pairwise relationships between numerical variables, excluding discrete integer variables with very few distinct values (`previousOwners` and `hasDamage`) since their scatter plots produce stacked points along horizontal or vertical lines, offering little meaningful insight.\n",
    "\n",
    "Although no strong linear relationships are observed in the numerical features of the training set, several patterns and logical trends emerge:\n",
    "- `Tax` and `paintQuality%` appear to have limited influence on the `price` (target), suggesting they may be less relevant features.\n",
    "\n",
    "- It seems a bit strange that the distribution of `paintQuality%` jumps so abruptly from values near zero to around 30, since it is a continuous variable and fairly well distributed across the remaining range. And `paintQuality%` does not appear to be related to any of the other variables shown here.\n",
    "\n",
    "- Cars with larger `engineSize` tend to have higher `price` which makes sense as larger engines are typically found in more powerful or luxury vehicles.\n",
    "\n",
    "- Most expensive cars tend to have lower `mileage`. However, many low-mileage vehicles are priced within more common, mid-range levels. As `mileage` increases, the average `price` tends to decrease and its variability also becomes smaller.\n",
    "\n",
    "- Most expensive cars are relatively recent and old cars tend to be cheaper. \n",
    "\n",
    "- For higher-priced cars, the `tax` values are clearly concentrated around a single point, which is quite below 200.\n",
    "\n",
    "- Cars paying higher `tax` rates tend to have lower `mpg`, a common trait of high-performance or luxury cars.\n",
    "\n",
    "- Older vehicles tend to have lower `mpg`, aligning with expectations about fuel efficiency over time.\n",
    "\n",
    "- More expensive cars tend to have lower `mpg`, as they often feature larger, more powerful engines that consume more fuel.\n",
    "\n",
    "- Once again, we can observe the presence of some luxury cars with low `mpg` and large `engineSizes`, but most vehicles have low `mpg` and small engines, corresponding to more standard models.\n",
    "\n",
    "- Finally, the impossible negative values observed in some variables are likely the result of simple sign errors, as their distributions closely mirror those of their corresponding positive values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filtered_t= [col for col in num if col not in ['previousOwners', 'hasDamage', 'price']]\n",
    "\n",
    "sns.pairplot(\n",
    "    X_test[num_filtered_t],\n",
    "    diag_kind='hist',\n",
    "    plot_kws={'color': palette[0], 'alpha': 0.6},\n",
    "    diag_kws={'color': palette[-2]}\n",
    ")\n",
    "plt.suptitle('Pairwise Relationship of Numerical Variables in Test Set', fontweight='bold', fontsize=16, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we already suspected, the test set exhibits the same relationships among the numerical variables as the training set, so it is unnecessary to repeat the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <div style=\"background-color:#e5c120ff; padding:1px; border-radius:10px;\">\n",
    "</div>\n",
    "\n",
    "Before looking at the inconsistencies, let's first check how many missing values we have in each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(train_df, val_df=None, test_df=None):\n",
    "\n",
    "    # Start with train data\n",
    "    missing_data_summary = pd.DataFrame({\n",
    "        \"Train n\": train_df.isnull().sum(),\n",
    "        \"Train %\": (train_df.isnull().mean() * 100).round(2)\n",
    "    })\n",
    "\n",
    "    # Add validation data if provided\n",
    "    if val_df is not None:\n",
    "        missing_data_summary[\"Validation n\"] = val_df.isnull().sum()\n",
    "        missing_data_summary[\"Validation %\"] = (val_df.isnull().mean() * 100).round(2)\n",
    "\n",
    "    # Add test data if provided\n",
    "    if test_df is not None:\n",
    "        missing_data_summary[\"Test n\"] = test_df.isnull().sum()\n",
    "        missing_data_summary[\"Test %\"] = (test_df.isnull().mean() * 100).round(2)\n",
    "\n",
    "    return missing_data_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data(car_eval, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can observe a high percentage of missing values in `mpg` and `tax`. In the training set, they are missing for 10.43% and 10.40% of the rows, respectively. This pattern is almost identical in the test set, with 10.10% and 10.16% missing. This might indicate that these variables have limited predictive value. If used, they will require robust imputation since removing 10% of the data is not feasible. <br>\n",
    "The consistency between the train and test sets is a positive sign, suggesting that the missingness is not random to a specific subset and that possibly an imputation strategy built on the training data will generalize well to the test data. The remaining variables show a small, consistent proportion of missing data (around 2%), which can be handled using standard imputation methods such as the median, mode, or kNN imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = car_eval.drop(columns=['price'])\n",
    "y = car_eval[['price']]\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y,\n",
    "                                                 test_size = 0.2,\n",
    "                                                 shuffle = True,\n",
    "                                                 random_state = 37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is essential to split the data into train and validation sets before performing steps such as missing value imputation or scaling, since the model should learn only from the training data. Otherwise, we would be introducing data leakage, which makes the model appear better than it actually is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec24-inconsistency-checks\" \n",
    "     style=\"background-color:#644712c5; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px #644712c5;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(242, 226, 9, 1);\"><b>2.4 | Inconsistency Checks</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique counts for each column\n",
    "unique_counts = car_eval.nunique()\n",
    "\n",
    "# Filter columns with less than 100 unique values\n",
    "filtered_col = [col for col in unique_counts.index if unique_counts[col] < 100]\n",
    "\n",
    "# Display unique values for filtered columns\n",
    "print(\"\\033[1mUnique values for each column:\\033[0m\")\n",
    "for col in filtered_col:\n",
    "    print(f\"{col} ({unique_counts[col]}): {car_eval[col].unique()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were already aware of many inconsistencies from the word cloud, but here we can get a clearer picture of them. In the next section, we will address the inconsistencies found in the training set and take the opportunity to create general consistency rules that can also be applied to the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec241-brand-model\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.1 |  Brand & model</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f9f9f9; border:1.5px solid #ccc; border-radius:8px; padding: 10px; width:1130px; text-align: justify;\">\n",
    "\n",
    "DECIDIMOS JUNTAR AS DUAS VARIÁVEIS PORQUE ESTÃO MUITO CORRELACIONADAS\n",
    "\n",
    "Regarding the variable `Brand`, we already know that there are many spelling errors and inconsistencies \n",
    "\n",
    "...\n",
    "\n",
    "Visualmente conseguimos identificar as allowed_brands que confirmamos que são de facto marcas de carros\n",
    "\n",
    "\n",
    "FAZER COM QUE O RETIRAR DO brand_model_dic SEJA LOGO NO PROJECT_DATA\n",
    "\n",
    "EXPLICAR PORQUÊ QUE FOI PRECISO FAZER O REPLACE DE MERCEDES-BENZ POR MERCEDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization function\n",
    "def norm(s):\n",
    "    if pd.isnull(s) or s == '':\n",
    "        return None\n",
    "    # Separate accents, lowercase, removes spaces\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s).strip().lower())\n",
    "    # Remove accents, keeping only the base characters\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    # Replace non-alphanumeric (e.g. punctuation) characters with a single space\n",
    "    return re.sub(r\"[^a-z0-9]+\", \" \", s).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = X_train.copy()\n",
    "X_val_norm = X_val.copy()\n",
    "X_test_norm = X_test.copy()\n",
    "brand_model_dic_norm = brand_model_dic.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*FAW-brand* and GAC-brand (e.g., *FAW Toyota*, *FAW VW*, *GAC Toyota*) refer to local Chinese joint ventures of global brands. Since they produce and sell essentially the same models as their parent brands, we dropped the *FAW-* and *GAC-* variants to simplify and speed up the matching process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows that have brand startting with \"FAW\" or \"GAC\"\n",
    "brand_model_dic_norm = brand_model_dic_norm[\n",
    "    ~brand_model_dic_norm['brand'].str.lower().str.startswith(('faw', 'gac'), na=False)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Normalize 'Brand' and 'model' in all datasets\n",
    "X_train_norm['Brand'] = X_train_norm['Brand'].apply(norm)\n",
    "X_train_norm['model'] = X_train_norm['model'].apply(norm)\n",
    "\n",
    "X_val_norm['Brand'] = X_val_norm['Brand'].apply(norm)\n",
    "X_val_norm['model'] = X_val_norm['model'].apply(norm)\n",
    "\n",
    "X_test_norm['Brand'] = X_test_norm['Brand'].apply(norm)\n",
    "X_test_norm['model'] = X_test_norm['model'].apply(norm)\n",
    "\n",
    "brand_model_dic_norm['Brand'] = brand_model_dic_norm['brand'].apply(norm)\n",
    "brand_model_dic_norm['model'] = brand_model_dic_norm['model'].apply(norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color:#e4b3c2ff; border:1.5px solid #e4b3c2ff; border-radius:8px; padding: 10px; width:1130px; text-align: justify;\">\n",
    "### APAGAR ### <br>\n",
    "VER A SOURCE DE FUZZY MATCHING E COMO FUNCIONA\n",
    "\n",
    "\n",
    "O FuzzyWuzzy funciona basicamente comparando strings de forma aproximada, ou seja, ele não exige que os textos sejam exatamente iguais. Ele usa Distância de Levenshtein, que mede o número mínimo de operações necessárias para transformar uma string na outra. As operações são: inserção, remoção ou substituição de caracteres. Com process.extract ou process.extractOne você pode procurar a string mais parecida em uma lista de opções.Retorna o texto mais próximo e a percentagem de correspondência.\n",
    "\n",
    "O fuzz.ratio() (usado pelo TheFuzz) calcula a semelhança percentual entre duas strings com base na distância de Levenshtein, ou seja, o número de inserções, deleções ou substituições necessárias para transformar uma string na outra. O valor é então normalizado para uma escala de 0 a 100.\n",
    "\n",
    "1. Impacto do tamanho da string <BR>\n",
    "Para strings curtas, mesmo uma pequena diferença significa uma grande variação percentual:<BR>\n",
    "Exemplo: cat vs bat <BR>\n",
    "Uma única letra diferente → distância de Levenshtein = 1 <BR>\n",
    "Similaridade = 1 - (1/3) = 66% <BR>\n",
    "Já temos 66%, mas com apenas uma letra diferente.<BR>\n",
    "Exemplo: an vs in <BR>\n",
    "Uma letra diferente → distância = 1 <BR>\n",
    "Similaridade = 1 - (1/2) = 50% <BR>\n",
    "Ou seja, uma única letra diferente em strings de 2 ou 3 caracteres pode derrubar a pontuação para 50% ou menos, mesmo que intuitivamente as duas strings sejam “quase iguais”. <BR>\n",
    "2. Consequência prática <BR>\n",
    "Se você exigir um corte mínimo de 50% para strings curtas, corre o risco de: <BR>\n",
    "Descartar matches que deveriam ser válidos, especialmente se forem abreviações, códigos ou nomes curtos.<BR>\n",
    "Criar falsos negativos: strings que são quase idênticas não são reconhecidas como tal.<BR>\n",
    "O comportamento se torna instável e pouco confiável para palavras curtas, pois a métrica percentual é muito sensível a pequenas alterações.<BR>\n",
    "3. Estratégias para contornar<BR>\n",
    "Reduzir o cutoff para strings curtas, como você fez (min_score = 0), permitindo qualquer correspondência.<BR>\n",
    "Usar outras métricas mais tolerantes a strings curtas, como partial_ratio ou token_set_ratio, dependendo do contexto.<BR>\n",
    "Normalizar ou expandir abreviações antes de comparar, quando possível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALLOWED_BRANDS = [\n",
    "    \"Volkswagen\",\"Toyota\",\"Audi\",\"Ford\",\"BMW\",\"Skoda\",\"Opel\",\"Mercedes-Benz\",\"Hyundai\"\n",
    "]\n",
    "brand_model_dic = brand_model_dic[brand_model_dic[\"brand\"].isin(ALLOWED_BRANDS)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- helpers curtos ----\n",
    "def norm(s: Optional[str]) -> str:\n",
    "    if s is None: return \"\"\n",
    "    s = unicodedata.normalize(\"NFKD\", str(s).strip().lower())\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    return re.sub(r\"[^a-z0-9]+\",\" \", s).strip()\n",
    "\n",
    "def fmatch(q: str, keys, cutoff=0.85):\n",
    "    if not q: return None\n",
    "    if q in keys: return q\n",
    "    best,score=None,0.0\n",
    "    for k in keys:\n",
    "        r = difflib.SequenceMatcher(None, q, k).ratio()\n",
    "        if r>score: best,score=k,r\n",
    "    return best if score>=cutoff else None\n",
    "\n",
    "# ---- 1) normalizar catálogo (só o necessário) ----\n",
    "bcol = next(c for c in brand_model_dic.columns if norm(c) in {\"brand\",\"marca\",\"make\"})\n",
    "mcol = next(c for c in brand_model_dic.columns if norm(c) in {\"model\",\"modelo\"})\n",
    "\n",
    "cat = brand_model_dic.copy()\n",
    "cat[bcol] = cat[bcol].astype(str).str.replace(r\"(?i)^mercedes[\\s-]*benz$\", \"Mercedes\", regex=True)\n",
    "\n",
    "# listas para fuzzy\n",
    "brand_keys_norm = sorted({norm(b) for b in cat[bcol].dropna().astype(str)})\n",
    "brand_norm2disp = {norm(b):str(b) for b in cat[bcol].dropna().astype(str)}\n",
    "\n",
    "# por marca, lista de modelos (normalizado -> display)\n",
    "models_by_brand = defaultdict(dict)\n",
    "global_models = {}\n",
    "brands_by_model_norm = defaultdict(set)  # inferir brand a partir do model\n",
    "for B,M in cat[[bcol,mcol]].dropna().itertuples(index=False):\n",
    "    nb, nm = norm(B), norm(M)\n",
    "    if nb and nm:\n",
    "        disp_brand = brand_norm2disp[nb]\n",
    "        models_by_brand[disp_brand][nm] = str(M)\n",
    "        global_models.setdefault(nm, str(M))\n",
    "        brands_by_model_norm[nm].add(disp_brand)\n",
    "\n",
    "# ---- 2) pequenos atalhos/patches ----\n",
    "BRAND_ALIAS = {\n",
    "    \"vw\":\"volkswagen\", \"v w\":\"volkswagen\", \"w\":\"volkswagen\",\n",
    "    \"mw\":\"bmw\", \"m w\":\"bmw\", \"bm\":\"bmw\", \"b m\":\"bmw\"   # MW/BM -> BMW\n",
    "}\n",
    "\n",
    "PREFERRED_BRAND_BY_MODEL = {\n",
    "    \"corsa\":\"Opel\",               # Corsa é Opel\n",
    "    \"shuttle\":\"Volkswagen\",\n",
    "    \"caddy maxi\":\"Volkswagen\",\n",
    "    \"yeti outdoor\":\"Skoda\",\n",
    "    \"slk\":\"Mercedes\",\n",
    "    \"zafira tourer\":\"Opel\",\n",
    "    \"up\":\"Volkswagen\",\n",
    "    \"u\":\"Volkswagen\",\n",
    "}\n",
    "\n",
    "SHORT_OK = {\"ka\",\"tt\",\"cc\",\"iq\",\"sl\",\"slk\",\"up\",\"u\"}\n",
    "def is_bad_short(model: str) -> bool:\n",
    "    q = norm(model)\n",
    "    if any(ch.isdigit() for ch in q):  # A3, Q5, 308...\n",
    "        return False\n",
    "    return (len(q) <= 2) and (q not in SHORT_OK)\n",
    "\n",
    "# ---- 3) aplicar ao train (fuzzy leve + patches) ----\n",
    "t_bcol = next(c for c in train.columns if norm(c) in {\"brand\",\"marca\",\"make\"})\n",
    "t_mcol = next(c for c in train.columns if norm(c) in {\"model\",\"modelo\"})\n",
    "\n",
    "bn_list, mn_list = [], []\n",
    "for b0, m0 in train[[t_bcol, t_mcol]].itertuples(index=False):\n",
    "    nb0, nm0 = norm(b0), norm(m0)\n",
    "\n",
    "    # brand: alias; senão fuzzy\n",
    "    nbq = BRAND_ALIAS.get(nb0, nb0)\n",
    "    b_key = fmatch(nbq, brand_keys_norm, cutoff=0.85)\n",
    "    b = brand_norm2disp.get(b_key, \"\") if b_key else \"\"\n",
    "\n",
    "    # model: tenta por marca; se falhar, tenta global\n",
    "    m = \"\"\n",
    "    if nm0:\n",
    "        if b and b in models_by_brand:\n",
    "            k = fmatch(nm0, list(models_by_brand[b].keys()), cutoff=0.85)\n",
    "            if k: m = models_by_brand[b][k]\n",
    "        if not m:\n",
    "            k = fmatch(nm0, list(global_models.keys()), cutoff=0.78)\n",
    "            if k:\n",
    "                m = global_models[k]\n",
    "                # se reconheci o modelo globalmente e a brand está vazia, inferir a brand\n",
    "                if not b:\n",
    "                    cands = list(brands_by_model_norm.get(k, []))\n",
    "                    if len(cands) == 1:\n",
    "                        b = cands[0]\n",
    "\n",
    "    # se não há brand mas o modelo denuncia a marca\n",
    "    if not b and nm0 in PREFERRED_BRAND_BY_MODEL:\n",
    "        b = PREFERRED_BRAND_BY_MODEL[nm0]\n",
    "        if not m: m = str(m0).strip()\n",
    "\n",
    "    # não deitar fora modelos curtos válidos\n",
    "    if m and is_bad_short(m):\n",
    "        m = \"\"\n",
    "\n",
    "    bn_list.append(b)\n",
    "    mn_list.append(m if m is not None else \"\")\n",
    "\n",
    "# >>> altera as colunas originais <<<\n",
    "train[t_bcol] = pd.Series(bn_list, index=train.index).fillna(\"\").astype(str)\n",
    "train[t_mcol] = pd.Series(mn_list, index=train.index).fillna(\"\").astype(str)\n",
    "\n",
    "print(\"Clean (light) feito — valores escritos em\", t_bcol, \"e\", t_mcol, \"(sem pós-processamento por modas).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec242-year\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.2 |  year</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the `year` variable, we know that the year must be a positive integer, so we will truncate any decimal values and take their absolute value. We also know that cars cannot have a year of registration later than 2020, which is the year of our database, so any values above 2020 will be clipped to 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'year' to integer and take absolute value; if NaN, keep as NaN\n",
    "X_train[\"year\"] = X_train[\"year\"].apply(lambda x: abs(int(x)) if pd.notnull(x) else x)\n",
    "\n",
    "# Cars registered after 2020 are set to 2020\n",
    "X_train.loc[(X_train[\"year\"] > 2020), \"year\"] = 2020\n",
    "\n",
    "# Apply the same transformation to the validation and test sets\n",
    "X_val[\"year\"] = X_val[\"year\"].apply(lambda x: abs(int(x)) if pd.notnull(x) else x)\n",
    "X_val.loc[(X_val[\"year\"] > 2020), \"year\"] = 2020\n",
    "\n",
    "X_test[\"year\"] = X_test[\"year\"].apply(lambda x: abs(int(x)) if pd.notnull(x) else x)\n",
    "X_test.loc[(X_test[\"year\"] > 2020), \"year\"] = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique years in training set:\", X_train['year'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec243-transmission\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.3 |  transmission</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the <code>transmission</code> variable, by combining domain knowledge with the unique values observed in the dataset, we identified the valid transmission types as <i>manual</i>, <i>automatic</i>, <i>semi-auto</i>, <i>unknown</i>, and <i>other</i>.  \n",
    "To correct typographical and formatting inconsistencies, we created a generic function named <code>correct</code>, which standardizes values (by lowercasing and trimming spaces) and uses fuzzy matching to map each entry to the closest valid category.  \n",
    "The <i>unknown</i> category is now treated as a legitimate label rather than missing data, preserving potentially meaningful information about unspecified transmission types.  \n",
    "This function can also be reused for other variables that present similar inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_categorical_value(input_value, valid_values, threshold=85, fallback_value=\"unknown\"):\n",
    "    # Start by normalizing the input to have a clean base for comparison\n",
    "    normalized_input = norm(input_value)\n",
    "\n",
    "    # If the input is empty or null, we can't match it, so return the fallback\n",
    "    if normalized_input is None:\n",
    "        return fallback_value\n",
    "\n",
    "    # We will find the best match from the list of valid values\n",
    "    best_score = 0\n",
    "    best_match = None\n",
    "\n",
    "    # Iterate through each option\n",
    "    for valid_value in valid_values:\n",
    "        # Calculate the similarity score\n",
    "        score = fuzz.ratio(normalized_input, valid_value)\n",
    "        \n",
    "        # If this score is the best we've seen so far, save it\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = valid_value\n",
    "            \n",
    "    # After checking all valid values, see if the best one we found is good enough\n",
    "    if best_score >= threshold:\n",
    "        return best_match\n",
    "    else:\n",
    "        return fallback_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_uniques = [norm('Manual'), norm('Automatic'), norm('Semi-Auto'), norm('Other')]\n",
    "\n",
    "# Correct 'transmission' in all datasets\n",
    "X_train['transmission'] = X_train['transmission'].apply(lambda x: correct_categorical_value(x, transmission_uniques))\n",
    "X_val['transmission'] = X_val['transmission'].apply(lambda x: correct_categorical_value(x, transmission_uniques))\n",
    "X_test['transmission'] = X_test['transmission'].apply(lambda x: correct_categorical_value(x, transmission_uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique transmission types in training set:\", X_train['transmission'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec244-mileage\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.4 |  mileage</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the `mileage` variable, we know that the it must be a positive value but we have some cases of negative mileage. For that reason we are going to na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% of negative, positive, nan and zero values in mileage \n",
    "mileage_negative = (car_eval['mileage'] < 0).mean() * 100\n",
    "mileage_positive = (car_eval['mileage'] > 0).mean() * 100 \n",
    "mileage_zero = (car_eval['mileage'] == 0).mean() * 100 \n",
    "mileage_nan = (car_eval['mileage'].isna()).mean() * 100 \n",
    "print(f\"Negative mileage values: {mileage_negative:.2f}%\") \n",
    "print(f\"Positive mileage values: {mileage_positive:.2f}%\") \n",
    "print(f\"Zero mileage values: {mileage_zero:.2f}%\") \n",
    "print(f\"NaN mileage values: {mileage_nan:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median, mean and standard deviation of cars' price for mileage <0 and 0<mileage<= abs(min(mileage)) \n",
    "median_mileage_neg = car_eval.loc[car_eval['mileage'] < 0, 'price'].median()\n",
    "mean_mileage_neg = car_eval.loc[car_eval['mileage'] < 0, 'price'].mean()\n",
    "std_mileage_neg = car_eval.loc[car_eval['mileage'] < 0, 'price'].std()\n",
    "median_mileage_0_min = car_eval.loc[(car_eval['mileage'] > 0) & (car_eval['mileage'] <= abs(car_eval['mileage'].min())), 'price'].median()\n",
    "mean_mileage_0_min = car_eval.loc[(car_eval['mileage'] > 0) & (car_eval['mileage'] <= abs(car_eval['mileage'].min())), 'price'].mean()\n",
    "std_mileage_0_min = car_eval.loc[(car_eval['mileage'] > 0) & (car_eval['mileage'] <= abs(car_eval['mileage'].min())), 'price'].std()\n",
    "print(\"Price stats for cars with mileage < 0:\")\n",
    "print(\"Median:\", median_mileage_neg, \"Mean:\", mean_mileage_neg, \"Std:\", std_mileage_neg)\n",
    "print(\"Price stats for cars with mileage between 0 and abs(min(mileage)):\")\n",
    "print(\"Median:\", median_mileage_0_min, \"Mean:\", mean_mileage_0_min, \"Std:\", std_mileage_0_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'mileage' to float; if negative, set to absolute value; if NaN, keep as NaN\n",
    "X_train[\"mileage\"] = X_train[\"mileage\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")\n",
    "\n",
    "# Do exactly the same for the validation and test set\n",
    "X_val[\"mileage\"] = X_val[\"mileage\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")\n",
    "X_test[\"mileage\"] = X_test[\"mileage\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec245-fueltype\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.5 |  fuelType</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the `fuelType` variable, by combining domain knowledge with the unique values observed in the dataset, we identified the possible valid fuel types as *petrol*, *diesel*, *hybrid*, *electric*, and *other*. To correct typographical and formatting errors, we applied the previously created `correct` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fueltype_uniques = [norm('Petrol'), norm('Diesel'), norm('Hybrid'), norm('Electric'), norm('Other')]\n",
    "\n",
    "# Correct 'fuelType' in all datasets\n",
    "X_train['fuelType'] = X_train['fuelType'].apply(lambda x: correct_categorical_value(x, fueltype_uniques))\n",
    "X_val['fuelType'] = X_val['fuelType'].apply(lambda x: correct_categorical_value(x, fueltype_uniques))\n",
    "X_test['fuelType'] = X_test['fuelType'].apply(lambda x: correct_categorical_value(x, fueltype_uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique fuel types in training set:\", X_train['fuelType'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec246-tax\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.6 |  tax</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upper bound (“max within reason”).**  \n",
    "To prevent extreme or erroneous values, we cap `tax` at a reasonable upper limit of **£5,490** (reflecting the highest charges applicable to the least fuel-efficient cars). Values above this threshold are set to 5,490.\n",
    "\n",
    "**Why are there floats if tax bands are discrete?**  \n",
    "Road-tax schedules are published in discrete bands, but fractional values can appear in real datasets due to data entry, currency conversion, proration (e.g., part-year charges), or prior preprocessing. We therefore **round to the nearest integer** after cleaning.\n",
    "\n",
    "**Zeros can be legitimate.**  \n",
    "A zero tax is plausible for multiple cases (e.g., vehicles used by a disabled person, historic vehicles, agricultural use, or certain zero-emission vehicles). We will **keep zeros** as valid observations.  \n",
    "(We will later check whether electric vehicles in our data align with zero or reduced tax.)\n",
    "\n",
    "**Negatives are invalid.**  \n",
    "Negative tax is not meaningful; such rows are treated as **errors** and handled (set to missing for imputation or clipped to 0, depending on downstream impact).\n",
    "\n",
    "**Summary of actions.**  \n",
    "1) clip high outliers to £5,490; 2) coerce to numeric; 3) drop/flag negatives; 4) round to integer.\n",
    "\n",
    "<span style=\"color:red;\">Explicar proque é que achamos que é uk, e ver se estamos a aplicar estas alteraçoes que dizemos aqui , voltar a colar  o urk </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% of negative, positive, nan and zero values in tax \n",
    "tax_negative = (car_eval['tax'] < 0).mean() * 100\n",
    "tax_positive = (car_eval['tax'] > 0).mean() * 100 \n",
    "tax_zero = (car_eval['tax'] == 0).mean() * 100 \n",
    "tax_nan = (car_eval['tax'].isna()).mean() * 100 \n",
    "print(f\"Negative tax values: {tax_negative:.2f}%\") \n",
    "print(f\"Positive tax values: {tax_positive:.2f}%\") \n",
    "print(f\"Zero tax values: {tax_zero:.2f}%\") \n",
    "print(f\"NaN tax values: {tax_nan:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median, mean and standard deviation of cars' price for tax <0 and 0<=tax<= abs(min(tax)) \n",
    "median_tax_neg = car_eval.loc[car_eval['tax'] < 0, 'price'].median()\n",
    "mean_tax_neg = car_eval.loc[car_eval['tax'] < 0, 'price'].mean()\n",
    "std_tax_neg = car_eval.loc[car_eval['tax'] < 0, 'price'].std()\n",
    "median_tax_0_min = car_eval.loc[(car_eval['tax'] >= 0) & (car_eval['tax'] <= abs(car_eval['tax'].min())), 'price'].median()\n",
    "mean_tax_0_min = car_eval.loc[(car_eval['tax'] >= 0) & (car_eval['tax'] <= abs(car_eval['tax'].min())), 'price'].mean()\n",
    "std_tax_0_min = car_eval.loc[(car_eval['tax'] >= 0) & (car_eval['tax'] <= abs(car_eval['tax'].min())), 'price'].std()\n",
    "print(\"Price stats for cars with tax < 0:\")\n",
    "print(\"Median:\", median_tax_neg, \"Mean:\", mean_tax_neg, \"Std:\", std_tax_neg)\n",
    "print(\"Price stats for cars with tax between 0 and abs(min(tax)):\")\n",
    "print(\"Median:\", median_tax_0_min, \"Mean:\", mean_tax_0_min, \"Std:\", std_tax_0_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'tax' to float; if negative, set to absolute value; if NaN, keep as NaN\n",
    "X_train[\"tax\"] = X_train[\"tax\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")\n",
    "\n",
    "# Do exactly the same for the validation and test set\n",
    "X_val[\"tax\"] = X_val[\"tax\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")\n",
    "X_test[\"tax\"] = X_test[\"tax\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec247-mpg\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.7 |  mpg</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mpg` variable represents the car’s fuel efficiency, measured as the average number of miles driven per gallon of fuel.  \n",
    "Higher values indicate greater efficiency and typically correlate with lower CO₂ emissions and lower running costs.  \n",
    "\n",
    "It is important to note that, for **electric vehicles**, the conversion to a “miles-per-gallon equivalent” (MPGe) is assumed to have already been performed in the dataset.  \n",
    "This means their `mpg` values are expressed on a comparable scale to those of conventional vehicles, allowing direct comparison across fuel types without additional adjustments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% of negative, positive, nan and zero values in mpg \n",
    "mpg_negative = (car_eval['mpg'] < 0).mean() * 100\n",
    "mpg_positive = (car_eval['mpg'] > 0).mean() * 100 \n",
    "mpg_zero = (car_eval['mpg'] == 0).mean() * 100 \n",
    "mpg_nan = (car_eval['mpg'].isna()).mean() * 100 \n",
    "print(f\"Negative mpg values: {mpg_negative:.2f}%\") \n",
    "print(f\"Positive mpg values: {mpg_positive:.2f}%\") \n",
    "print(f\"Zero mpg values: {mpg_zero:.2f}%\") \n",
    "print(f\"NaN mpg values: {mpg_nan:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'mpg' to float; if negative, set to absolute value; if NaN, keep as NaN\n",
    "X_train[\"mpg\"] = X_train[\"mpg\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")\n",
    "\n",
    "# Do exactly the same for the validation and test set\n",
    "X_val[\"mpg\"] = X_val[\"mpg\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")\n",
    "X_test[\"mpg\"] = X_test[\"mpg\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec248-enginesize\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.8 |  engineSize</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `engineSize` variable represents the engine’s displacement in liters (dm³).  \n",
    "Since this value corresponds to the total cylinder volume of the engine, a value of **0** would indicate an engine with no physical displacement — which is mechanically impossible, even for electric vehicles.  \n",
    "\n",
    "In the case of **electric cars**, manufacturers often record the engine size as a nominal value (e.g., 0.0 or missing) simply to maintain consistency with datasets originally designed for combustion engines.  \n",
    "Therefore, zero values are not valid physical measurements but placeholders that we may later treat as missing or encode as a separate category to distinguish electric vehicles from those with combustion engines.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% of negative, positive, nan and zero values in engineSize \n",
    "engineSize_negative = (car_eval['engineSize'] < 0).mean() * 100\n",
    "engineSize_positive = (car_eval['engineSize'] > 0).mean() * 100 \n",
    "engineSize_zero = (car_eval['engineSize'] == 0).mean() * 100 \n",
    "engineSize_nan = (car_eval['engineSize'].isna()).mean() * 100 \n",
    "print(f\"Negative engineSize values: {engineSize_negative:.2f}%\") \n",
    "print(f\"Positive engineSize values: {engineSize_positive:.2f}%\") \n",
    "print(f\"Zero engineSize values: {engineSize_zero:.2f}%\") \n",
    "print(f\"NaN engineSize values: {engineSize_nan:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all engineSize values==0 to NaN\n",
    "X_train.loc[X_train['engineSize'] == 0, 'engineSize'] = np.nan\n",
    "X_val.loc[X_val['engineSize'] == 0, 'engineSize'] = np.nan\n",
    "X_test.loc[X_test['engineSize'] == 0, 'engineSize'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median, mean and standard deviation of cars' price for engineSize <0 and 0<engineSize<= abs(min(engineSize)) \n",
    "median_engineSize_neg = car_eval.loc[car_eval['engineSize'] < 0, 'price'].median()\n",
    "mean_engineSize_neg = car_eval.loc[car_eval['engineSize'] < 0, 'price'].mean()\n",
    "std_engineSize_neg = car_eval.loc[car_eval['engineSize'] < 0, 'price'].std()\n",
    "median_engineSize_0_min = car_eval.loc[(car_eval['engineSize'] >= 0) & (car_eval['engineSize'] <= abs(car_eval['engineSize'].min())), 'price'].median()\n",
    "mean_engineSize_0_min = car_eval.loc[(car_eval['engineSize'] >= 0) & (car_eval['engineSize'] <= abs(car_eval['engineSize'].min())), 'price'].mean()\n",
    "std_engineSize_0_min = car_eval.loc[(car_eval['engineSize'] >= 0) & (car_eval['engineSize'] <= abs(car_eval['engineSize'].min())), 'price'].std()\n",
    "print(\"Price stats for cars with engineSize < 0:\")\n",
    "print(\"Median:\", median_engineSize_neg, \"Mean:\", mean_engineSize_neg, \"Std:\", std_engineSize_neg)\n",
    "print(\"Price stats for cars with engineSize between 0 and abs(min(engineSize)):\")\n",
    "print(\"Median:\", median_engineSize_0_min, \"Mean:\", mean_engineSize_0_min, \"Std:\", std_engineSize_0_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'engineSize' to float; if negative, set to absolute value; if NaN, keep as NaN\n",
    "X_train[\"engineSize\"] = X_train[\"engineSize\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")\n",
    "\n",
    "# Do exactly the same for the validation and test set\n",
    "X_val[\"engineSize\"] = X_val[\"engineSize\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")\n",
    "X_test[\"engineSize\"] = X_test[\"engineSize\"].apply(\n",
    "    lambda x: float(x) if pd.notnull(x) and x >= 0 else (abs(float(x)) if pd.notnull(x) else np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec249-paintquality\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.9 |  paintQuality%</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É UM POUCO ESTRANHO QUE OS UNIQUE VALUES PASSEM DE APROXIMADAMENTE 3.25 LOGO PARA 30 SENDO QUE DEPOIS TEMOS VALORES EM PRATICAMENTE TODAS AS PERCENTAGENS E APENAS ALGUNS UNIQUE VALUES SÃO CONTÍNUOS MAS PARA ALÉM DESTA MERA OBSERVAÇÃO NÃO HÁ NADA QUE NOS INDIQUE QUE ESTES VALORES SÃO DE FACTO INCORRETOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of cases with paintQuality% between 0-100, NaN and beyond 100\n",
    "paint_quality_valid = ((X_train['paintQuality%'] >= 0) & (X_train['paintQuality%'] <= 100)).mean() * 100\n",
    "paint_quality_nan = (X_train['paintQuality%'].isna()).mean() * 100\n",
    "paint_quality_out_of_range = ((X_train['paintQuality%'] < 0) | (X_train['paintQuality%'] > 100)).mean() * 100\n",
    "print(f\"Valid paintQuality (0-100): {paint_quality_valid:.2f}%\")\n",
    "print(f\"NaN paintQuality: {paint_quality_nan:.2f}%\")\n",
    "print(f\"Out of range paintQuality (<0 or >100): {paint_quality_out_of_range:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median, mean and standard deviation of cars' price for paintQuality% >100 and 99<=paintQuality%<= 100\n",
    "median_paint_above_100 = car_eval.loc[car_eval['paintQuality%'] >100, 'price'].median()\n",
    "mean_paint_above_100 = car_eval.loc[car_eval['paintQuality%'] >100, 'price'].mean()\n",
    "std_paint_above_100 = car_eval.loc[car_eval['paintQuality%'] >100, 'price'].std()\n",
    "median_paint_99_100 = car_eval.loc[(car_eval['paintQuality%'] >= 99) & (car_eval['paintQuality%'] <= 100), 'price'].median()\n",
    "mean_paint_99_100 = car_eval.loc[(car_eval['paintQuality%'] >= 99) & (car_eval['paintQuality%'] <= 100), 'price'].mean()\n",
    "std_paint_99_100 = car_eval.loc[(car_eval['paintQuality%'] >= 99) & (car_eval['paintQuality%'] <= 100), 'price'].std()\n",
    "print(\"Price stats for cars with paintQuality > 100:\")\n",
    "print(\"Median:\", median_paint_above_100, \"Mean:\", mean_paint_above_100, \"Std:\", std_paint_above_100)\n",
    "print(\"Price stats for cars with paintQuality between 99 and 100:\")\n",
    "print(\"Median:\", median_paint_99_100, \"Mean:\", mean_paint_99_100, \"Std:\", std_paint_99_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip 'paintQuality%' to be within [0, 100]; if NaN, keep as NaN\n",
    "X_train[\"paintQuality%\"] = (\n",
    "    pd.to_numeric(X_train[\"paintQuality%\"], errors=\"coerce\")\n",
    "      .clip(lower=0, upper=100)                             \n",
    ")\n",
    "\n",
    "# Do exactly the same for the validation and test set\n",
    "X_val[\"paintQuality%\"] = (\n",
    "    pd.to_numeric(X_val[\"paintQuality%\"], errors=\"coerce\")\n",
    "      .clip(lower=0, upper=100)                             \n",
    ")\n",
    "X_test[\"paintQuality%\"] = (\n",
    "    pd.to_numeric(X_test[\"paintQuality%\"], errors=\"coerce\")\n",
    "      .clip(lower=0, upper=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec2410-previous-owners\"\n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.10 |  previousOwners</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'previousOwners' to int; if negative, set to absolute value; if NaN, keep as NaN\n",
    "X_train[\"previousOwners\"] = X_train[\"previousOwners\"].apply(\n",
    "    lambda x: int(x) if pd.notnull(x) and x >= 0 else (abs(int(x)) if pd.notnull(x) else np.nan)\n",
    ")\n",
    "\n",
    "# Do exactly the same for the validation and test set\n",
    "X_val[\"previousOwners\"] = X_val[\"previousOwners\"].apply(\n",
    "    lambda x: int(x) if pd.notnull(x) and x >= 0 else (abs(int(x)) if pd.notnull(x) else np.nan)\n",
    ")\n",
    "X_test[\"previousOwners\"] = X_test[\"previousOwners\"].apply(\n",
    "    lambda x: int(x) if pd.notnull(x) and x >= 0 else (abs(int(x)) if pd.notnull(x) else np.nan)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec2411-hasdamage\" \n",
    "     style=\"background-color:#e5c120ff; padding:15px; border-radius:10px;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(255, 255, 255, 1);\"><b>2.4.11 |  hasDamage</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median, mean and standard deviation of cars' price for hasDamage=0 and hasDamage is missing\n",
    "median_hasDamage_0 = car_eval.loc[car_eval['hasDamage'] == 0, 'price'].median()\n",
    "mean_hasDamage_0 = car_eval.loc[car_eval['hasDamage'] == 0, 'price'].mean()\n",
    "std_hasDamage_0 = car_eval.loc[car_eval['hasDamage'] == 0, 'price'].std()\n",
    "median_hasDamage_nan = car_eval.loc[car_eval['hasDamage'].isna(), 'price'].median()\n",
    "mean_hasDamage_nan = car_eval.loc[car_eval['hasDamage'].isna(), 'price'].mean()\n",
    "std_hasDamage_nan = car_eval.loc[car_eval['hasDamage'].isna(), 'price'].std()\n",
    "print(\"Price stats for cars with hasDamage = 0:\")\n",
    "print(\"Median:\", median_hasDamage_0, \"Mean:\", mean_hasDamage_0, \"Std:\", std_hasDamage_0)\n",
    "print(\"Price stats for cars with hasDamage is missing:\")\n",
    "print(\"Median:\", median_hasDamage_nan, \"Mean:\", mean_hasDamage_nan, \"Std:\", std_hasDamage_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_hasdamage_0 = car_eval.loc[car_eval['hasDamage'] ==0, 'price']\n",
    "price_hasdamage_na = car_eval.loc[car_eval['hasDamage'].isna(), 'price']\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(price_hasdamage_0, bins=30, alpha=0.6, label='hasDamage = 0', color=palette[1], density=True)\n",
    "plt.hist(price_hasdamage_na, bins=30, alpha=0.6, label='hasDamage is missing', color=palette[-3], density=True)\n",
    "\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Price Distribution by Damage Status')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROVAVELMENTE ESTA VARIÁVEL HASDAMAGE DEVIA SER DESCARTADA PORQUE NÃO PARECE TER IMPACTO NO PREÇO AO CONTRÁRIO DO QUE SERIA DE ESPERAR, TALVEZ TENHA SIDO RECOLHIDO DE FORMA INCORRETA. DE QUALQUER FORMA VAMOS DEIXAR QUE O PRÓPRIO MODELO FAÇA ESSA DECISÃO POR SI MESMO NA FASE DE FEATURE SELECTION."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e5c120ff; padding:1px; border-radius:10px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que já lidámos com a inconsistências é possível que tenhamos novos duplicados, por isso vamos verificar novamente a sua existência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates in the training and validation sets\n",
    "print(X_train.duplicated().sum())\n",
    "print(X_val.duplicated().sum())\n",
    "#Print these rows\n",
    "duplicates_train = X_train[X_train.duplicated(keep=False)]\n",
    "duplicates_val = X_val[X_val.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates without price, keeping the last which doesn't really matter since we have already shuffled the data\n",
    "X_train_no_duplicate = X_train.drop_duplicates(keep='last')\n",
    "X_val_no_duplicate = X_val.drop_duplicates(keep='last')\n",
    "\n",
    "# Index to keep \n",
    "index_train = X_train_no_duplicate.index\n",
    "index_val = X_val_no_duplicate.index\n",
    "\n",
    "X_train = X_train.loc[index_train]\n",
    "y_train = y_train.loc[index_train]\n",
    "\n",
    "X_val = X_val.loc[index_val]\n",
    "y_val = y_val.loc[index_val]\n",
    "\n",
    "# Get the shape of the datasets\n",
    "print(\"X_train shape after removing duplicates:\", X_train.shape)\n",
    "print(\"y_train shape after removing duplicates:\", y_train.shape)\n",
    "print(\"X_val shape after removing duplicates:\", X_val.shape)\n",
    "print(\"y_val shape after removing duplicates:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec25-missing-data\"\n",
    "     style=\"background-color:#644712c5; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px #644712c5;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(242, 226, 9, 1);\"><b>2.5 | Missing Data</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all unknown values in 'transmission', 'fuelType' columns to NaN\n",
    "X_train['transmission'] = X_train['transmission'].replace('unknown', np.nan)\n",
    "X_val['transmission'] = X_val['transmission'].replace('unknown', np.nan)\n",
    "X_test['transmission'] = X_test['transmission'].replace('unknown', np.nan)  \n",
    "X_train['fuelType'] = X_train['fuelType'].replace('unknown', np.nan)\n",
    "X_val['fuelType'] = X_val['fuelType'].replace('unknown', np.nan)\n",
    "X_test['fuelType'] = X_test['fuelType'].replace('unknown', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data(X_train,X_val,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rows that have at least 40% missing values\n",
    "threshold = 0.4\n",
    "missing_rows_train = X_train[X_train.isna().mean(axis=1) >= threshold]\n",
    "missing_rows_val = X_val[X_val.isna().mean(axis=1) >= threshold]\n",
    "missing_rows_test = X_test[X_test.isna().mean(axis=1) >= threshold]\n",
    "display(missing_rows_train)\n",
    "display(missing_rows_val)\n",
    "display(missing_rows_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como não há casos em train nem val não fazemos delete, mas já dá para ver que há linhas no teste com tanto missing que será muito difícil acertar a não ser que missing seja indício de algo..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este artigo, \"The Missing Data Widespread Problem\", argumenta que métodos antigos e simples para lidar com dados em falta, como apagar linhas com valores em falta (listwise deletion) ou preencher com a média (mean imputation), são perigosos. Eles podem introduzir viés (bias), reduzir o poder estatístico dos modelos e levar a conclusões completamente erradas. <br>\n",
    "A contribuição mais importante foi classificar os dados em falta pelo motivo pelo qual estão em falta. Existem 3 categorias: <br><br>\n",
    "\n",
    "1. MCAR (Missing Completely At Random): <br>\n",
    " A ausência de um dado não tem absolutamente nenhuma relação com qualquer outra variável no dataset, nem com o próprio valor que está em falta. <br>\n",
    " **sol:** É o caso mais fácil. O artigo menciona que, se a percentagem de dados em falta for muito pequena (ex: <5%), apagar as linhas pode ser aceitável, pois a amostra restante ainda é representativa. <br><br>\n",
    "2. MAR (Missing At Random): <br>\n",
    "O que é? Este é o conceito mais importante e muitas vezes mal compreendido. A ausência de um dado pode ser explicada por outras variáveis que nós observámos no dataset, mas não pelo valor do próprio dado em falta. <br>\n",
    "Não podemos simplesmente apagar as linhas, pois isso iria enviesar o nosso dataset. No entanto, como a \"missingness\" está relacionada com dados que temos, podemos usar esses dados para preencher os valores em falta de forma inteligente. O artigo defende fortemente o uso de métodos modernos como a Imputação Múltipla por Equações Encadeadas (MICE) para este cenário.<br><br>\n",
    "3. MNAR (Missing Not At Random): <br>\n",
    "O que é? A ausência de um dado está diretamente relacionada com o valor do próprio dado que está em falta.<br>\n",
    "Este é o pior cenário. É muito difícil de tratar porque a razão da ausência está em dados que não temos. Requer modelos e suposições muito mais complexos. ACREDITAMOS QUE NONOSSO CASO ISTO NÃO OCORRE<br><br><br>\n",
    "\n",
    "O artigo para além de propor Little's Test for MCAR também sugere uma abordagem prática que vamos aplicar: criar uma variável \"dummy\" (1 para em falta, 0 para presente) e depois usar testes t (para variáveis numéricas) e testes qui-quadrado (para variáveis categóricas) para ver se a \"missingness\" está correlacionada com outras variáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A função test_missingness_Xtrain serve para diagnosticar o mecanismo de missingness dos dados, ou seja, para determinar se os dados em falta são MCAR, MAR ou MNAR. Implementa a abordagem prática sugerida no paper.\n",
    "Para uma coluna com dados em falta (passa a ser a nossa target), a função testa se essa \"missingness\" está estatisticamente relacionada com todas as outras colunas do dataset.\n",
    "(O teste não consegue distinguir os 3 tipos diretamente. O seu principal trabalho é distinguir MCAR de MAR. Ele não consegue provar ou refutar MNAR.)\n",
    "\n",
    "Testes-t para Preditores Numéricos: Compara a média de uma variável numérica (ex: mileage) para o grupo onde mpg está em falta vs. o grupo onde mpg está presente. Se as médias forem muito diferentes (p-valor < 0.05), significa que a \"missingness\" de mpg está relacionada com mileage.\n",
    "\n",
    "\n",
    "Testes Qui-quadrado para Preditores Categóricos: Compara a distribuição de uma variável categórica (ex: fuelType) para o grupo onde transmission está em falta vs. o grupo onde está presente. Se as distribuições forem muito diferentes (p-valor < 0.05), significa que a \"missingness\" de transmission está relacionada com fuelType. <br>\n",
    "Se nenhum teste der um p-valor significativo, podemos assumir MCAR. Se pelo menos um teste der um p-valor significativo, temos evidências de que os dados são MAR.<BR><BR>\n",
    "\n",
    "EXPLICAR QUE O MICE TALVEZ FOSSE UMA ABORDAGEM MELHOR POR LIDAR COM A INCERTEZA E TUDO MAIS E TALVEZ (DEPENDENDO SE O NOTEBOOK ESTOVER LENTO OU NÃO...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The imputation method is decided case by case (variable by variable)\n",
    "\n",
    "# Logic (Little & Rubin, 2019)\n",
    "# Step 1: Evaluate pattern of missing relation to other variables (missing mecanism) \n",
    "\n",
    "# MCAR (Missing Completely at Random):\n",
    "# Missingness independent of observed/unobserved data: safe to impute with central tendency (median/mean/mode).\n",
    "# MAR (Missing At Random):\n",
    "# Missingness explained by another variable (e.g., tax missing for electric cars): impute within group (conditionally) / KNN / Or MICE\n",
    "# MNAR (Missing Not At Random):\n",
    "# Depends on unobserved values (e.g., cars too new for tax data): possibly removal or specific enconding\n",
    "\n",
    "# Step 2: Consider Missing Proportion relative to Dataset and Datatype\n",
    "# Step 3: Apply method\n",
    "\n",
    "\n",
    "\n",
    "#Se a lista de variáveis com p-valor < 0.05 (sig_vars) não estiver vazia, classificamos como MAR. Caso contrário, é MCAR.\n",
    "\n",
    "# O paper recomenda MICE, mas o KNNImputer é outro método de imputação multivariada excelente e conceitualmente semelhante. \n",
    "# #Ele funciona sob a premissa de MAR: se a \"missingness\" pode ser explicada por outras variáveis, então podemos usar essas outras variáveis\n",
    "# para encontrar as linhas mais \"parecidas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Test MAR/MCAR: For a given target column with missing data, test if the \"missingness\" is statistically related to all other columns in the dataset\n",
    "\n",
    "def test_missingness_Xtrain(X_train, target, num_cols):\n",
    "\n",
    "    # Create missingness indicator (1 = missing, 0 = not missing)\n",
    "    miss_col = target + '_missing'\n",
    "    X_train[miss_col] = X_train[target].isna().astype(int)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    #  Numeric predictors:  t-tests\n",
    "    for col in numeric_cols:\n",
    "        if col != target and col in X_train.columns:\n",
    "            group0 = X_train.loc[X_train[miss_col] == 0, col].dropna()\n",
    "            group1 = X_train.loc[X_train[miss_col] == 1, col].dropna()\n",
    "            if len(group0) > 1 and len(group1) > 1:\n",
    "                p = ttest_ind(group0, group1, equal_var=False)[1]\n",
    "                results.append((target, col, 't-test', p))\n",
    "\n",
    "    #  Categorical predictors:  chi-square\n",
    "    cat_cols = X_train.select_dtypes(exclude='number').columns\n",
    "    for col in cat_cols:\n",
    "        if col != target:\n",
    "            table = pd.crosstab(X_train[col], X_train[miss_col])\n",
    "            if table.shape[0] > 1:\n",
    "                _, p, _, _ = chi2_contingency(table)\n",
    "                results.append((target, col, 'chi-square', p))\n",
    "    \n",
    "    # Remove helper column\n",
    "    X_train.drop(columns=[miss_col], inplace=True)\n",
    "    \n",
    "    return pd.DataFrame(results, columns=['Target', 'Variable', 'Test', 'p_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling MAR/MCAR test function on all variables with missing values\n",
    "\n",
    "# Identify variables that have missing values\n",
    "vars_with_missing = [col for col in X_train.columns if X_train[col].isna().sum() > 0]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for var in vars_with_missing:\n",
    "    results = test_missingness_Xtrain(X_train, var, numeric_cols)\n",
    "    sig_vars = results[results['p_value'] < 0.05]['Variable'].tolist()\n",
    "    status = \"MAR\" if len(sig_vars) > 0 else \"MCAR\"\n",
    "    all_results.append({\n",
    "        'Target': var,\n",
    "        'Missing_Count': X_train[var].isna().sum(),\n",
    "        'Missing_%': X_train[var].isna().mean() * 100,\n",
    "        'Significant_Variables': sig_vars,\n",
    "        'Status': status\n",
    "    })\n",
    "\n",
    "# Combine into one summary DataFrame\n",
    "summary = pd.DataFrame(all_results).sort_values('Missing_%', ascending=False)\n",
    "summary.reset_index(drop=True, inplace=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "# Clean up columns\n",
    "summary['Missing_Count'] = summary['Missing_Count'].astype(int)\n",
    "summary['Missing_%'] = summary['Missing_%'].round(2)\n",
    "summary['Significant_Variables'] = summary['Significant_Variables'].apply(\n",
    "    lambda lst: ', '.join(lst) if isinstance(lst, list) and len(lst) > 0 else '—'\n",
    ")\n",
    "\n",
    "# Reorder + sort + reset index\n",
    "summary = (\n",
    "    summary[['Target', 'Missing_Count', 'Missing_%', 'Status', 'Significant_Variables']]\n",
    "    .sort_values(['Missing_%', 'Target'], ascending=[False, True])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnóstico: Use X_train + y_train. (Pode olhar para as respostas para entender o problema).\n",
    "# Pré-processamento e Imputação: Use APENAS X_train. (Não pode usar as respostas para se preparar para o exame).\n",
    "# NÁO SEI SE FAZ SENTIDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAR: A ausência do dado está estatisticamente relacionada com outras variáveis no seu dataset. \n",
    "# Existe um padrão, e a coluna Significant_Variables diz-lhe qual é esse padrão.\n",
    "\n",
    "# Matematicamente, comparar a média de uma variável 0/1 entre dois grupos é equivalente a comparar as proporções,\n",
    "# e o teste t é robusto o suficiente para esta tarefa. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`transmission`, `paintQuality%`, `previousOwners`, `Brand`, `model`: <BR>\n",
    "Os testes não encontraram nenhuma relação estatística entre a falta de dados nestas colunas e qualquer outra variável no seu dataset.\n",
    "Interpretação Prática:\n",
    "É provavelmente o resultado de erros esporádicos de entrada de dados. A sua ausência é \"ruído\" aleatório nos dados.\n",
    "Ação Recomendada: Para dados MCAR, a imputação multivariada (KNN/MICE) continua a ser a opção mais segura e consistente, pois preserva as linhas de dados. No entanto, usar um método mais simples como a imputação pela moda (para transmission, Brand, model) ou pela mediana (para as outras) seria estatisticamente mais defensável aqui do que nos casos MAR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mpg`, `tax`, `engineSize`, `fuelType`, `hasDamage`, `year`, `mileage`: <BR>\n",
    "MAR.  Cada uma destas variáveis tem pelo menos uma \"Significant Variable\", o que prova que a sua ausência não é aleatória.\n",
    "Interpretação Prática (por subgrupo):\n",
    "`mpg e tax`: Continuam a ser as mais problemáticas (mais de 10% em falta). A sua ausência está fortemente ligada a todas as características de base do carro (year, Brand, model, transmission, fuelType). Isto confirma um padrão sistemático, provavelmente relacionado com a fonte de dados para certos tipos de carros.\n",
    "`engineSize`: A sua ausência está relacionada com mpg, model e transmission. Esta é uma relação de via única importante. Significa que saber o tipo de transmission de um carro ajuda a prever se o engineSize estará em falta. (Mas o inverso, como vimos no Grupo 1, não é verdade). Isto faz sentido: talvez para certos modelos com uma transmission específica (ex: elétrica), o campo engineSize não seja aplicável ou não seja preenchido.\n",
    "`fuelType`: A sua ausência está ligada a previousOwners e model. Esta é uma relação interessante. Pode sugerir que para certos modelos, especialmente aqueles com um maior número de donos anteriores, o registo do tipo de combustível se torna menos fiável e é deixado em branco.\n",
    "`hasDamage`, `mileage`, `year`: A ausência destas está maioritariamente ligada a model, o que sugere que a qualidade e a completude da recolha de dados varia dependendo do modelo do carro.\n",
    "Ação Recomendada: Para todas estas variáveis MAR, a imputação multivariada (KNNImputer) é fortemente recomendada. O algoritmo irá usar estas relações que você descobriu para fazer um preenchimento muito mais inteligente do que qualquer método simples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling (MinMaxScaler): O KNN funciona com base em distâncias. É fundamental escalar as variáveis numéricas para que uma variável com uma escala grande (como mileage) não domine o cálculo da distância. O seu código faz isto na perfeição, escalando antes da imputação e revertendo a escala depois (inverse_transform).\n",
    "#O MinMaxScaler garante que todas as variáveis numéricas têm exatamente o mesmo peso no cálculo da distância, tornando a definição de \"vizinho mais próximo\" muito mais equilibrada e justa. É preferível para algoritmos baseados em distância porque preserva a forma da distribuição original e garante que os limites (0 e 1) são os mesmos para todas as variáveis.\n",
    "\n",
    "\n",
    "# Encoding (OneHotEncoder): O KNN não funciona com texto. O seu código converte as variáveis categóricas para um formato numérico (one-hot encoding) antes de as imputar, o que é a abordagem correta.\n",
    "# O One-Hot Encoding trata cada categoria como uma característica distinta. A distância entre um carro a Petrol ([1, 0, 0]) e um a Diesel ([0, 1, 0]) é calculada de forma geométrica e correta no espaço de características. A diferença entre eles é simplesmente que são diferentes, sem qualquer noção de \"maior\" ou \"menor\".\n",
    "\n",
    "# KNN não é feito baseado nas significant_variables, mas sim em todas as variáveis pois é mais robusto usar todas as outras variáveis disponíveis para a imputação com KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical variables\n",
    "all_categorical_vars = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# Separating Numerical from Categorical Variables where Missing pattern is MAR\n",
    "MAR_numerical_vars = ['mpg', 'tax', 'engineSize', 'hasDamage', 'year', 'mileage']\n",
    "MAR_categorical_vars = ['fuelType']\n",
    "all_mar_vars = MAR_numerical_vars + MAR_categorical_vars\n",
    "\n",
    "# Columns where Missing pattern is  MCAR\n",
    "mcar_vars = [col for col in X_train.columns if col not in all_mar_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VAMOS USAR ORDINALENCODER PORQUE COM OHE OS MISSINGS DE MAR_categorical_vars IRAM SER PREENCHIDOS POR ZERO E PORTANTO QUANDO FOSSE FAZER KNN IMPUTER NÃO IRIA IDENTIFCAR OS MISSINGS DOS MESMOS. TENDO EM CONSIDERAÇÃO QUE TAMBÉM SÓ TEMOS UMA VARIÁVEL EM MAR_categorical_vars NÃO VAI CRIAR MUITA DIFERENÇA/ENVIESAMENTO USANDO ORDINALENCODER... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APAGAR: Lida com categorias que possam aparecer no conjunto de validação/teste mas não no de treino.\n",
    "# Copy datasets to avoid modifying originals\n",
    "X_train_processed = X_train.copy()\n",
    "X_val_processed = X_val.copy()\n",
    "X_test_processed = X_test.copy()\n",
    "\n",
    "# Transform categorical variables using OneHotEncoder before KNN imputation\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan, encoded_missing_value=np.nan)\n",
    "\n",
    "# Fit on training data and transform training, validation, and test data\n",
    "X_train_processed[all_categorical_vars] = ordinal_encoder.fit_transform(X_train[all_categorical_vars])\n",
    "X_val_processed[all_categorical_vars] = ordinal_encoder.transform(X_val[all_categorical_vars])\n",
    "X_test_processed[all_categorical_vars] = ordinal_encoder.transform(X_test[all_categorical_vars])\n",
    "\n",
    "# Save the column names and indices for later\n",
    "original_cols = X_train_processed.columns\n",
    "train_index = X_train.index\n",
    "val_index = X_val.index\n",
    "test_index = X_test.index\n",
    "\n",
    "# Initialize MinMaxScaler for scaling\n",
    "scaler = MinMaxScaler()\n",
    "# fit on training data and transform training, validation, and test data\n",
    "X_train_scaled = scaler.fit_transform(X_train_processed)\n",
    "X_val_scaled = scaler.transform(X_val_processed)\n",
    "X_test_scaled = scaler.transform(X_test_processed)\n",
    "\n",
    "#Initialize KNNImputer for imputation\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "# Fit on training data and transform training, validation, and test data\n",
    "X_train_imputed_scaled = knn_imputer.fit_transform(X_train_scaled)\n",
    "X_val_imputed_scaled = knn_imputer.transform(X_val_scaled)\n",
    "X_test_imputed_scaled = knn_imputer.transform(X_test_scaled)\n",
    "\n",
    "# Inverse transform to get back to original scale\n",
    "X_train_imputed = scaler.inverse_transform(X_train_imputed_scaled)\n",
    "X_val_imputed = scaler.inverse_transform(X_val_imputed_scaled)\n",
    "X_test_imputed = scaler.inverse_transform(X_test_imputed_scaled)\n",
    "\n",
    "# Reconstruct DataFrames with original columns and indices\n",
    "X_train_final = pd.DataFrame(X_train_imputed, index=train_index, columns=original_cols)\n",
    "X_val_final = pd.DataFrame(X_val_imputed, index=val_index, columns=original_cols)\n",
    "X_test_final = pd.DataFrame(X_test_imputed, index=test_index, columns=original_cols)\n",
    "\n",
    "# Round categorical variables to nearest integer (ex: KNN imputer can produce fuelType=1.4 which doesn't correspond to any category, so we round to 1)\n",
    "\n",
    "# Inverse transform categorical variables back to original categories\n",
    "for df in [X_train_final, X_val_final, X_test_final]:\n",
    "    for col in ['year', 'previousOwners']:\n",
    "        # Ensure the column exists before rounding\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].round().astype(int)\n",
    "    df[all_categorical_vars] = df[all_categorical_vars].round()\n",
    "    df[all_categorical_vars] = ordinal_encoder.inverse_transform(df[all_categorical_vars])\n",
    "\n",
    "# Combine MCAR variables (original, no imputation) with MAR variables (imputed)\n",
    "X_train_final = pd.concat([X_train[mcar_vars], X_train_final[all_mar_vars]], axis=1)\n",
    "X_val_final = pd.concat([X_val[mcar_vars], X_val_final[all_mar_vars]], axis=1)\n",
    "X_test_final = pd.concat([X_test[mcar_vars], X_test_final[all_mar_vars]], axis=1)\n",
    "\n",
    "# Ensure the final datasets have the same column order as the original datasets\n",
    "X_train = X_train_final[X_train.columns]\n",
    "X_val = X_val_final[X_val.columns]\n",
    "X_test = X_test_final[X_test.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data(X_train,X_val, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR ALL THE OTHER VARIABLES (MCAR) WE CAN IMPUTE WITH MEDIAN/MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the model more frequent corresponds to the brand more frequent so that we don't create any inconsistency when imputing missing values\n",
    "mode_model = X_train['model'].mode()[0]\n",
    "mode_brand = X_train['Brand'].mode()[0]\n",
    "print(\"Mode of model:\", mode_model)\n",
    "print(\"Mode of brand:\", mode_brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values with median or mode\n",
    "for col in mcar_vars:\n",
    "    if X_train[col].dtype in ['float64', 'int64']: # Numerical variables\n",
    "        median = X_train[col].median()\n",
    "        X_train[col].fillna(median, inplace=True)\n",
    "        X_val[col].fillna(median, inplace=True)\n",
    "        X_test[col].fillna(median, inplace=True)\n",
    "    else:  # Categorical variables\n",
    "        mode = X_train[col].mode()[0]\n",
    "        X_train[col].fillna(mode, inplace=True)\n",
    "        X_val[col].fillna(mode, inplace=True)\n",
    "        X_test[col].fillna(mode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data(X_train,X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGORA QUE IMPUTAMOS MAIS VALORES PODEMOS TER MAIS DUPLICADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicates in the training and validation sets\n",
    "print(X_train.duplicated().sum())\n",
    "print(X_val.duplicated().sum())\n",
    "#Print these rows\n",
    "duplicates_train = X_train[X_train.duplicated(keep=False)]\n",
    "duplicates_val = X_val[X_val.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates without price, keeping the last which doesn't really matter since we have already shuffled the data\n",
    "X_train_no_duplicate = X_train.drop_duplicates(keep='last')\n",
    "\n",
    "# Index to keep \n",
    "index_train = X_train_no_duplicate.index\n",
    "\n",
    "X_train = X_train.loc[index_train]\n",
    "y_train = y_train.loc[index_train]\n",
    "\n",
    "# Get the shape of the datasets\n",
    "print(\"X_train shape after removing duplicates:\", X_train.shape)\n",
    "print(\"y_train shape after removing duplicates:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe numerical columns with additional statistics (skewness and kurtosis)\n",
    "numeric_desc_t = X_train.select_dtypes(include='number').describe().T\n",
    "numeric_desc_t[['skew', 'kurtosis']] = numeric_desc_t.agg(['skew', 'kurtosis']).T\n",
    "numeric_desc_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe numerical columns with additional statistics (skewness and kurtosis)\n",
    "numeric_desc_t = X_val.select_dtypes(include='number').describe().T\n",
    "numeric_desc_t[['skew', 'kurtosis']] = numeric_desc_t.agg(['skew', 'kurtosis']).T\n",
    "numeric_desc_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe numerical columns with additional statistics (skewness and kurtosis)\n",
    "numeric_desc_t = X_val.select_dtypes(include='number').describe().T\n",
    "numeric_desc_t[['skew', 'kurtosis']] = numeric_desc_t.agg(['skew', 'kurtosis']).T\n",
    "numeric_desc_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESCREVER ALGO SOBRE A COMPARAÇÃO ENTRE O QUE HAVIA ANTES E AGORA DEPOIS DE LIDAR COM INCONSISTÊNCIAS E MISSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic describe for categorical variables\n",
    "cat_train_desc = X_train.select_dtypes(include='object').describe().T\n",
    "\n",
    "# Add proportion of the most frequent category\n",
    "cat_train_desc['top_freq_ratio'] = cat_train_desc['freq'] / cat_train_desc['count']\n",
    "cat_train_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_val_desc = X_val.select_dtypes(include='object').describe().T\n",
    "cat_val_desc['top_freq_ratio'] = cat_val_desc['freq'] / cat_val_desc['count']\n",
    "cat_val_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_test_desc = X_test.select_dtypes(include='object').describe().T\n",
    "cat_test_desc['top_freq_ratio'] = cat_test_desc['freq'] / cat_test_desc['count']\n",
    "cat_test_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESCREVER ALGO SOBRE A COMPARAÇÃO ENTRE O QUE HAVIA ANTES E AGORA DEPOIS DE LIDAR COM INCONSISTÊNCIAS E MISSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEPOIS DE TRATAR DOS MISSINGS DE YEAR É QUE PODEMOS DEIXAR A INT64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'previousOwners' to int64\n",
    "X_train['previousOwners'] = X_train['previousOwners'].astype('int64')\n",
    "X_val['previousOwners'] = X_val['previousOwners'].astype('int64')\n",
    "X_test['previousOwners'] = X_test['previousOwners'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec26-explore-data-visualizations-continued\" \n",
    "     style=\"background-color:#644712c5; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px #644712c5;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(242, 226, 9, 1);\"><b>2.6 | Explore Data Visualizations (Continued)</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin_comparison(X_train, X_val, X_test, cat_col, num_cols, top_n=None):\n",
    "    # Organize into a dictionary for convenience\n",
    "    dfs_dict = {'Train': X_train, 'Validation': X_val, 'Test': X_test}\n",
    "    n_rows = len(num_cols)\n",
    "    # Create subplots: 3 columns (Train, Val, Test) and rows equal to number of numerical columns\n",
    "    fig, axes = plt.subplots(n_rows, 3,\n",
    "                             figsize=(3 * 5.5, n_rows * 5),\n",
    "                             squeeze=False) # Ensures axes is always a 2D array\n",
    "\n",
    "   \n",
    "    if top_n is not None:\n",
    "        # If top_n is a number, use nlargest\n",
    "        category_order = X_train[cat_col].value_counts().nlargest(top_n).index\n",
    "        title_detail = f'Top {top_n} {cat_col.capitalize()}s'\n",
    "    else:\n",
    "        # If top_n is None, get all unique categories and sort them\n",
    "        category_order = sorted(X_train[cat_col].unique())\n",
    "        title_detail = f'All {cat_col.capitalize()}s'\n",
    "\n",
    "    # Iterate through each numerical column to create a new row of plots\n",
    "    for row_idx, num_col in enumerate(num_cols):\n",
    "        # Iterate through the three dataframes for the columns\n",
    "        for col_idx, (df_name, df) in enumerate(dfs_dict.items()):\n",
    "            ax = axes[row_idx, col_idx]\n",
    "            # Filter the dataframe to only include the top N categories\n",
    "            df_filtered = df[df[cat_col].isin(category_order)]\n",
    "\n",
    "            sns.violinplot(\n",
    "                data=df_filtered,\n",
    "                x=num_col,\n",
    "                y=cat_col,\n",
    "                order=category_order,\n",
    "                ax=ax,\n",
    "                palette='YlOrBr',\n",
    "                inner='quartile',\n",
    "                linewidth=1.2\n",
    "            )\n",
    "            \n",
    "            ax.set_title(f'{num_col} Distribution ({df_name} Set)', fontweight='bold', fontsize=12)\n",
    "            ax.set_xlabel(num_col.capitalize(), fontsize=10)\n",
    "            \n",
    "            if col_idx == 0:\n",
    "                ax.set_ylabel(cat_col.capitalize(), fontsize=10)\n",
    "            else:\n",
    "                ax.set_ylabel('')\n",
    "                \n",
    "            sns.despine(ax=ax, top=True, right=True)\n",
    "\n",
    "    plt.suptitle(f'Distribution Comparison for {title_detail}', \n",
    "                 fontweight='bold', fontsize=18, y=1.03)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_num_features = ['engineSize', 'mpg', 'tax']\n",
    "plot_violin_comparison(X_train, X_val, X_test, 'Brand', some_num_features, top_n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribuição de \"engineSize\" (Tamanho do Motor):\n",
    "Concentração em Motores Pequenos: Marcas como Ford, Opel, VW, Skoda e Toyota mostram uma forte concentração de veículos com motores pequenos, geralmente entre 1.0L e 2.0L. O formato \"gordo\" do violino nessas áreas indica que a grande maioria dos seus carros à venda se enquadra nessa faixa.\n",
    "Marcas Premium com Maior Variedade: Mercedes-Benz, BMW e Audi exibem distribuições muito mais amplas. Embora também tenham modelos com motores de 2.0L, eles possuem uma presença significativa de motores maiores (3.0L ou mais), o que é característico de marcas de luxo/performance.\n",
    "Outlier da BMW: A BMW tem uma \"cauda\" muito longa no seu gráfico, indicando a presença de modelos com motores excecionalmente grandes em comparação com as outras marcas.\n",
    "Distribuição de \"mpg\" (Milhas por Galão - Eficiência de Combustível):\n",
    "Eficiência vs. Potência: Há uma relação inversa clara com o tamanho do motor. Marcas com motores menores (Skoda, Toyota, Ford) mostram distribuições de mpg concentradas em valores mais altos, indicando maior eficiência de combustível.\n",
    "Menor Eficiência em Marcas Premium: Mercedes-Benz e BMW têm distribuições de mpg mais concentradas em valores mais baixos, o que é esperado devido aos seus motores maiores e mais focados em performance.\n",
    "Variedade da Audi e VW: Audi e VW apresentam uma boa dispersão, mostrando que oferecem tanto modelos muito eficientes quanto modelos mais potentes e menos eficientes.\n",
    "Distribuição de \"tax\" (Imposto):\n",
    "Impostos Ligados ao Motor: O imposto (tax) parece estar fortemente correlacionado com o engineSize. Marcas com motores maiores (Mercedes-Benz, BMW, Audi) apresentam uma distribuição de impostos muito mais ampla e com medianas mais altas.\n",
    "Faixa de Imposto Padrão: Muitas marcas (Ford, VW, Opel, Toyota, Skoda) têm uma forte concentração de veículos numa faixa de imposto específica (em torno de 150), provavelmente correspondendo a uma categoria de emissões/motor popular nesses países.\n",
    "Valores Extremos: Novamente, as marcas premium mostram caudas mais longas, indicando a existência de modelos com impostos muito elevados, enquanto marcas como Skoda e Opel quase não têm veículos nas faixas de imposto mais altas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_comparison(X_train, X_val, X_test, 'transmission', some_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMENTÁRIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_comparison(X_train, X_val, X_test,'fuelType', some_num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMENTÁRIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot_by_category(df, num_col, cat_col, top_n=None, sort_by_median=True, ascending=False):\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    df_plot = df.copy()\n",
    "\n",
    "    # Filter for the top N most frequent categories if top_n is specified\n",
    "    if top_n is not None:\n",
    "        top_categories = df_plot[cat_col].value_counts().nlargest(top_n).index\n",
    "        df_plot = df_plot[df_plot[cat_col].isin(top_categories)]\n",
    "        title_prefix = f'Top {top_n} '\n",
    "    else:\n",
    "        title_prefix = ''\n",
    "\n",
    "    # Determine the order of categories on the plot\n",
    "    order = None\n",
    "    if sort_by_median:\n",
    "        # Order categories by the median for better visualization\n",
    "        order = df_plot.groupby(cat_col)[num_col].median().sort_values(ascending=ascending).index\n",
    "\n",
    "    # Create the boxplot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.boxplot(\n",
    "        data=df_plot,\n",
    "        x=cat_col,\n",
    "        y=num_col,\n",
    "        order=order,\n",
    "        palette=\"YlOrBr\",\n",
    "        width=0.6\n",
    "    )\n",
    "\n",
    "    # Set titles and labels\n",
    "    plt.title(f'Distribution of {num_col.capitalize()} by {title_prefix}{cat_col.capitalize()}',\n",
    "              fontweight='bold', fontsize=16)\n",
    "    plt.xlabel(cat_col.capitalize(), fontsize=12)\n",
    "    plt.ylabel(num_col.capitalize(), fontsize=12)\n",
    "    \n",
    "    # Adjustments for better readability\n",
    "    plt.xticks(rotation=25, ha='right')\n",
    "    sns.despine(top=True, right=True)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot_by_category(X_train, num_col='engineSize', cat_col='fuelType')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMENTÁRIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_relationship(df, cat_col1, cat_col2):\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    \n",
    "    ax = sns.countplot(\n",
    "        data=df,\n",
    "        x=cat_col1,\n",
    "        hue=cat_col2,\n",
    "        palette='YlOrBr'\n",
    "    )\n",
    "    \n",
    "    # Pre-calculate the total for each category on the x-axis (ex: total for 'diesel', 'petrol', ...)\n",
    "    totals = df.groupby(cat_col1).size()\n",
    "\n",
    "    # Iterate over each bar in the plot to add the annotation\n",
    "    for p in ax.patches:\n",
    "        # Get the height of the bar (the count)\n",
    "        height = p.get_height()\n",
    "        \n",
    "        # Ignore bars with no height (NaN or 0)\n",
    "        if pd.isna(height) or height == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get the name of the x-axis group this bar belongs to\n",
    "        group_name = ax.get_xticklabels()[int(p.get_x() + p.get_width() / 2.)].get_text()\n",
    "        \n",
    "        # If the group name exists in our calculated totals\n",
    "        if group_name in totals:\n",
    "            # Calculate the correct percentage\n",
    "            percentage = f'{(height / totals[group_name]) * 100:.1f}%'\n",
    "            \n",
    "            # Add the text label to the center of the bar\n",
    "            ax.text(\n",
    "                p.get_x() + p.get_width() / 2., \n",
    "                height / 2,                    \n",
    "                percentage,\n",
    "                ha='center',           \n",
    "                va='center',                   \n",
    "                color='black',                \n",
    "                fontweight='bold',\n",
    "                fontsize=9\n",
    "            )\n",
    "    \n",
    "    # Set titles and labels\n",
    "    ax.set_title(f'Relationship between {cat_col1.capitalize()} and {cat_col2.capitalize()}', fontweight='bold', fontsize=16)\n",
    "    ax.set_xlabel(cat_col1.capitalize(), fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "    plt.legend(title=cat_col2.capitalize(), loc='upper right')\n",
    "    sns.despine(top=True, right=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_relationship(X_train, 'fuelType', 'transmission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_hexbin_year_mileage(df):\n",
    "#     \"\"\"\n",
    "#     Cria um gráfico de densidade 2D (hexbin) para mostrar a concentração\n",
    "#     de dados na relação entre ano e quilometragem.\n",
    "#     \"\"\"\n",
    "#     # Usamos o jointplot do seaborn que é excelente para isso\n",
    "#     g = sns.jointplot(\n",
    "#         data=df,\n",
    "#         x='year',\n",
    "#         y='mileage',\n",
    "#         kind='hex',      # Tipo de gráfico principal\n",
    "#         cmap='YlOrBr',   # Mapa de cores\n",
    "#         gridsize=30,     # Número de hexágonos (ajuste conforme necessário)\n",
    "#         height=8\n",
    "#     )\n",
    "    \n",
    "#     g.fig.suptitle('Densidade da Relação Ano vs. Quilometragem', fontweight='bold', y=1.02, fontsize=16)\n",
    "#     g.set_axis_labels('Ano de Fabrico', 'Quilometragem (Mileage)', fontsize=12)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Como usar:\n",
    "# plot_hexbin_year_mileage(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For cars with tax=0, get the percentage that are hybrid, petrol, ...\n",
    "zero_tax_counts = X_train[X_train['tax'] == 0]['fuelType'].value_counts(normalize=True) * 100\n",
    "print(\"Percentage of each fuel type among vehicles with zero tax:\")\n",
    "print(zero_tax_counts)\n",
    "\n",
    "# For cars with tax (>0), get the percentage that are hybrid, petrol, ...\n",
    "nonzero_tax_counts = X_train[X_train['tax'] > 0]['fuelType'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage of each fuel type among vehicles with non-zero tax:\")\n",
    "print(nonzero_tax_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Electric accounts for a miniscule proportion, so only the existence of electric cars do not justify the zeros. SERÁ QUE OS HÍBRIDOS TAMBÉM NÃO PAGAM TAX?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of each transmission\n",
    "transmission_counts = X_train['transmission'].value_counts(normalize=True) * 100\n",
    "print(\"Percentage of each transmission type:\")\n",
    "print(transmission_counts)\n",
    "# Percentage of each transmission for electric vehicles\n",
    "electric_transmission_counts = X_train[X_train['fuelType'] == 'electric']['transmission'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPercentage of each transmission type among electric vehicles:\")\n",
    "print(electric_transmission_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMENTÁRIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variáveis fortemente correlacionadas entre si (ex: engineSize e tax) podem causar multicolinearidade — convém verificar com VIF ou corr().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec27-outliers\" \n",
    "     style=\"background-color:#644712c5; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px #644712c5;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(242, 226, 9, 1);\"><b>2.7 | Outliers</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOGO NO EDA TINHAMOS VISTO VÁRIOS OUTLIERS. OS OUTLIERS QUE SÃO ERROS (Error Outliers) JÁ FORAM IDENTIFICADOS E CORRIGIDOS NA PARTE DAS INCONSISTÊNCIAS. AQUI VAMOS FOCAR-NOS NOS OUTLIERS QUE SÃO VÁLIDOS (Valid Outliers) E DECIDIR O QUE FAZER COM ELES."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAD É uma excelente alternativa ao método IQR e é considerado o equivalente robusto do método do Z-score (desvio-padrão). Em vez de usar a média e o desvio-padrão (que são sensíveis a outliers), ele usa a mediana e o desvio absoluto mediano, que são muito mais resistentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outlier_detection(df, columns, iqr_factor=1.5, mad_threshold=3.5, return_indices=False):\n",
    "\n",
    "    # Just to ensure columns is a list\n",
    "    if isinstance(columns, str):\n",
    "        columns = [columns]\n",
    "    \n",
    "    results_list = []\n",
    "    all_indices = {'iqr': set(), 'mad': set(), 'both': set()}\n",
    "\n",
    "    # Iterate through each column\n",
    "    for column in columns:\n",
    "        if column not in df.columns:\n",
    "            print(f\"Column '{column}' not found in the DataFrame. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        data_series = df[column]\n",
    "\n",
    "        ### IQR Method Calculations ###\n",
    "        Q1 = data_series.quantile(0.25)\n",
    "        Q3 = data_series.quantile(0.75)\n",
    "        IQR = Q3-Q1\n",
    "        iqr_lower_bound = Q1- (iqr_factor * IQR)\n",
    "        iqr_upper_bound = Q3+ (iqr_factor * IQR)\n",
    "        iqr_outliers_filter = (data_series < iqr_lower_bound) | (data_series > iqr_upper_bound)\n",
    "        iqr_outlier_indices = set(df[iqr_outliers_filter].index)\n",
    "        \n",
    "        ### MAD Method Calculations ###\n",
    "        median = data_series.median()\n",
    "        mad = (data_series - median).abs().median()\n",
    "        \n",
    "        # Handle case where MAD is zero\n",
    "        if mad == 0:\n",
    "            mad_lower_bound = median\n",
    "            mad_upper_bound = median\n",
    "            # If MAD is zero, consider all values as outliers that are not equal to the median\n",
    "            mad_outliers_filter = (data_series != median)\n",
    "        else:\n",
    "            mad_lower_bound = median - (mad_threshold * mad)\n",
    "            mad_upper_bound = median + (mad_threshold * mad)\n",
    "            mad_outliers_filter = (data_series < mad_lower_bound) | (data_series > mad_upper_bound)\n",
    "        \n",
    "        mad_outlier_indices = set(df[mad_outliers_filter].index)\n",
    "\n",
    "        # Comparison Calculations\n",
    "        both_indices = iqr_outlier_indices.intersection(mad_outlier_indices)\n",
    "        \n",
    "        column_summary = {\n",
    "            'Column': column,\n",
    "            'Outlier Count (IQR)': len(iqr_outlier_indices),\n",
    "            'Outlier % (IQR)': round(len(iqr_outlier_indices) / len(df) * 100, 2),\n",
    "            'Outlier Count (MAD)': len(mad_outlier_indices),\n",
    "            'Outlier % (MAD)': round(len(mad_outlier_indices) / len(df) * 100, 2),\n",
    "            'Outlier Count (Both)': len(both_indices),\n",
    "            'Outlier % (Both)': round(len(both_indices) / len(df) * 100, 2),\n",
    "        }\n",
    "        results_list.append(column_summary)\n",
    "\n",
    "        # Store indices if requested\n",
    "        if return_indices:\n",
    "            all_indices['iqr'].update(iqr_outlier_indices)\n",
    "            all_indices['mad'].update(mad_outlier_indices)\n",
    "                    \n",
    "    if return_indices:\n",
    "        all_indices['both'] = all_indices['iqr'].intersection(all_indices['mad'])\n",
    "        for key in all_indices:\n",
    "            all_indices[key] = sorted(list(all_indices[key]))\n",
    "\n",
    "    summary_df = pd.DataFrame(results_list).set_index('Column')\n",
    "    \n",
    "    if return_indices:\n",
    "        return summary_df, all_indices\n",
    "        \n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outliers= num.drop(['hasDamage', 'price'])\n",
    "outliers_train, indices_train = compare_outlier_detection(X_train, num_outliers, return_indices=True)\n",
    "outliers_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_hist(X_train, num_outliers, \"in the Training Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of rows that are outliers by both IQR and MAD\n",
    "indices_to_remove = indices_train['both']\n",
    "\n",
    "# Calculate the impact of removing these rows\n",
    "count_to_remove = len(indices_to_remove)\n",
    "total_rows = len(X_train)\n",
    "percentage_to_remove = (count_to_remove / total_rows) * 100 if total_rows > 0 else 0\n",
    "\n",
    "print(f\"\\nIf we were to remove all rows identified as outliers by both the IQR and MAD methods:\")\n",
    "print(f\" We would remove {count_to_remove} rows. This represents {percentage_to_remove:.2f}% of the entire dataset ({count_to_remove} out of {total_rows} rows).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_outlier_detection(X_val, num_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_outlier_detection(X_test, num_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estes outliers parecem ser valores legítimos já que são valores possíveis e estão presentes em X_train, X_val e X_test, pelo que não nos faz sentido removê-los até porque se o fizéssemos estaríamos a perder muita informação que é relevante para o modelo iria enviesar a sua perceção da verdadeira variabilidade dos dados. Também não nos faz sentido aplicar o método de Winsorization, que consiste em limitar os valores extremos aos limites definidos, porque estaríamos a distorcer a verdadeira distribuição dos dados. Consideramos que desde que sejam usados métodos robustos para criar o modelo, estes outliers não irão afetar negativamente o desempenho do mesmo.\n",
    "\n",
    "Em vez de esperar que um modelo frágil funcione em dados \"limpos\", a nossa estratégia passa por usar um modelo forte (Random Forest, KNN) em dados realistas.\n",
    "FITNESS F.:\n",
    "\n",
    "Huber Loss (A escolha padrão e mais recomendada):<BR>\n",
    "Como funciona: É um híbrido. Para erros pequenos, comporta-se como o Erro Quadrático Médio (MSE), sendo eficiente. Para erros grandes (causados por outliers), comporta-se como o Erro Absoluto Médio (MAE), sendo robusto.\n",
    "Como usar: Use modelos que a implementam diretamente, como HuberRegressor em scikit-learn.\n",
    "Erro Absoluto Médio (MAE ou L1 Loss):<BR>\n",
    "Como funciona: Penaliza o erro linearmente, não quadraticamente. Um outlier tem uma influência proporcional, não desproporcional.\n",
    "Como usar: Use modelos como Lasso ou SGDRegressor(loss='epsilon_insensitive').\n",
    "Use Procedimentos de Treino Robustos:<BR>\n",
    "RANSAC (Random Sample Consensus): Ideal para dados com uma grande proporção de outliers. Ele ajusta o modelo a pequenos subconjuntos dos dados e encontra o consenso da maioria, ignorando os pontos que não se encaixam. Use RANSACRegressor em scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SE QUISERMOS PODEMOS TESTAR FAZER CLIP DE ALGUNS OUTLIERS E VER SE MELHORA O MODELO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para uma variável numérica, um outlier é um valor de magnitude extrema (muito alto ou muito baixo). Para uma variável categórica, não existe magnitude. Portanto, um outlier categórico é uma categoria que ocorre com uma frequência muito baixa no conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequency of each brand in the training set\n",
    "brand_frequencies=X_train['Brand'].value_counts(normalize=True)\n",
    "brand_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the brands that are above the threshold\n",
    "common_brands = brand_frequencies[brand_frequencies > 0.01].index.tolist()\n",
    "print(f\"Found {len(common_brands)} common brands in X_train:\", common_brands)\n",
    "\n",
    "# Apply the Transformation to all Datasets\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    # The mask identifies any brand that is not in our list of common_brands\n",
    "    mask = ~df['Brand'].isin(common_brands)\n",
    "    # Replace those brands with 'other_brand'\n",
    "    df.loc[mask, 'Brand'] = 'other_brand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_frequencies=X_train['model'].value_counts(normalize=True)\n",
    "model_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the models that are above the threshold\n",
    "common_models = model_frequencies[model_frequencies > 0.0001].index.tolist()\n",
    "print(f\"Found {len(common_models)} common models in X_train\")\n",
    "\n",
    "# Apply the Transformation to all Datasets\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    # The mask identifies any model that is not in our list of common_models\n",
    "    mask = ~df['model'].isin(common_models)\n",
    "    # Replace those models with 'other_model'\n",
    "    df.loc[mask, 'model'] = 'other_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VANTAGENS E DESVANTAGENS DE FAZER ISTO <BR>\n",
    "Reduz a dimensionalidade: Diminui o número de categorias O QUE É BOM SE QUISERMOS USAR One-Hot Encoding. <BR>\n",
    "Estabiliza o modelo: A categoria \"Outro\" terá dados suficientes para que o modelo aprenda um padrão mais estável.<BR>\n",
    "Lida com categorias novas: Se no futuro aparecer uma nova categoria que não estava no treino, pode ser automaticamente classificada como \"Outro\".<BR><BR>\n",
    "Desvantagens:<BR>\n",
    "Você perde a informação específica contida nas categorias raras (embora essa informação fosse provavelmente ruído)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec28-create-extra-features\" \n",
    "     style=\"background-color:#644712c5; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px #644712c5;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(242, 226, 9, 1);\"><b>2.8 | Create Extra Features</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A relação entre a car_age e o price é muito mais linear e fácil de o modelo aprender do que a relação entre year e price. Um carro de 1 ano é diferente de um de 2 anos. Um carro de 2022 é diferente de um de 2021. É a mesma informação, mas apresentada de uma forma mais intuitiva para o modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable 'age' considering that the dataset provided is from 2020\n",
    "X_train['age'] = 2020 - X_train['year']\n",
    "X_val['age'] = 2020 - X_val['year']\n",
    "X_test['age'] = 2020 - X_test['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relação Quilometragem/Idade\n",
    "Uma quilometragem alta num carro novo é diferente de uma quilometragem alta num carro antigo.\n",
    "Como Criar: df['km_per_year'] = df['mileage'] / (df['car_age'] + 1) (Adicionamos +1 para evitar a divisão por zero para carros com menos de 1 ano de idade).\n",
    "Porquê? Esta variável captura a intensidade de uso do carro. Um km_per_year muito alto pode indicar um carro que foi usado para fins comerciais ou que sofreu muito desgaste, o que influencia o preço."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable mileage per age\n",
    "X_train['miles_per_year'] = X_train['mileage'] / (X_train['age'] + 1) # +1 to avoid division by zero\n",
    "X_val['miles_per_year'] = X_val['mileage'] / (X_val['age'] + 1)\n",
    "X_test['miles_per_year'] = X_test['mileage'] / (X_test['age'] + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Principal Benefício: Esta é a vantagem mais importante. A combinação brand_model captura a interação única entre a marca e o modelo que os modelos lineares (como Regressão Linear, Ridge, Lasso) teriam muita dificuldade em aprender.<br><br>\n",
    "Redução de Ambiguidade\n",
    "O Problema: Alguns nomes de modelos podem ser partilhados por diferentes marcas. Por exemplo, o \"Corsa\" existe na Opel, Vauxhall e Chevrolet. Se você tratar model como uma característica separada, o modelo pode ficar confuso e atribuir um efeito médio a \"Corsa\" que não reflete a realidade de nenhuma das marcas. <br><br>\n",
    "Simplificação para Modelos de Árvore\n",
    "Modelos baseados em árvores de decisão funcionam dividindo os dados. Ter uma única característica brand_model muito informativa pode permitir que a árvore faça divisões muito poderosas logo no início. Por exemplo, uma das primeiras regras na árvore poderia ser: IF brand_model IS 'Porsche_911' THEN ... ELSE .... Isto pode levar a árvores mais simples e, por vezes, mais eficazes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable that combines brand and model\n",
    "X_train['brand_model'] = X_train['Brand'] + '_' + X_train['model']\n",
    "X_val['brand_model'] = X_val['Brand'] + '_' + X_val['model']\n",
    "X_test['brand_model'] = X_test['Brand'] + '_' + X_test['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLARO QUE ESTAS VARIÁVEIS CRIADAS ESTÃO FORTEMENTE CORRELACIONADAS COM AS VARIÁVEIS ORIGINAIS, O QUE REALÇA A IMPORTÂNCIA DA FASE DE FEATURE SELECTION PARA NÃO AS MANTER TODAS E ACABAR POR TER MULTICOLINEARIDADE NO MODELO FINAL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec29-categorical-variables\" \n",
    "     style=\"background-color:#644712c5; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px #644712c5;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(242, 226, 9, 1);\"><b>2.9 | Encoding Categorical Variables</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplicidade e Rapidez de Implementação:<BR>\n",
    "É extremamente fácil de calcular (basicamente um value_counts() e um .map()). Em termos computacionais, é muito mais rápido do que outros métodos, especialmente em grandes conjuntos de dados.<BR><BR>\n",
    "Não Aumenta a Dimensionalidade:<BR>\n",
    "Esta é a sua maior vantagem em comparação com o One-Hot Encoding (OHE). Se você tem uma variável com 1000 categorias diferentes (uma variável de alta cardinalidade, como código_postal ou ID_produto), o OHE criaria 1000 novas colunas. O Frequency Encoding cria apenas uma nova coluna, mantendo o seu conjunto de dados compacto e evitando a \"maldição da dimensionalidade\".<BR><BR>\n",
    "Captura a \"Popularidade\" ou Importância da Categoria:<BR>\n",
    "A frequência de uma categoria pode ser, por si só, uma feature preditiva. Por exemplo, num modelo de deteção de fraude, transações de \"comerciantes\" muito raros podem ser mais suspeitas. Num modelo de vendas, os \"produtos\" mais frequentes são provavelmente os mais populares. O Frequency Encoding injeta esta informação diretamente no modelo.<BR><BR>\n",
    "Funciona Excecionalmente Bem com Modelos Baseados em Árvores:<BR>\n",
    "Algoritmos como Random Forest, XGBoost e LightGBM são mestres em encontrar os melhores pontos de corte em variáveis numéricas. Ao converter as categorias em frequências (números), você dá à árvore a capacidade de criar regras como: \"Se a frequência da Brand for < 0.05 (ou seja, uma marca rara), então o risco é maior\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "categorical_cols = X_train.select_dtypes(include='object').columns.tolist()\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to hold the frequency map for each categorical column\n",
    "all_frequency_maps = {}\n",
    "\n",
    "# frequencies can be learned only from the training set\n",
    "for col in categorical_cols:\n",
    "    # Calculate the frequency and convert to a dictionary\n",
    "    frequency_map = X_train[col].value_counts(normalize=True).to_dict()\n",
    "    # Store the learned map\n",
    "    all_frequency_maps[col] = frequency_map\n",
    "    print(f\"Learned map for '{col}': {frequency_map}\")\n",
    "\n",
    "# Make copies to work on, preserving the originals\n",
    "X_train_final = X_train.copy()\n",
    "X_val_final = X_val.copy()\n",
    "X_test_final = X_test.copy()\n",
    "\n",
    "# Apply the Maps to Transform the Data\n",
    "for col in categorical_cols:\n",
    "    # Get the specific map for this column\n",
    "    frequency_map = all_frequency_maps[col]\n",
    "\n",
    "    # Identify columns that are encoded\n",
    "    new_col_name = f\"{col}_freq_enc\"\n",
    "    \n",
    "    # Apply the map to all three dataframes\n",
    "    # For val/test, use .apply() with a lambda function and .get(x, 0)\n",
    "    # to handle categories that were not in the training set (although we tried to avoid that earlier)\n",
    "    X_train_final[new_col_name] = X_train_final[col].map(frequency_map)\n",
    "    X_val_final[new_col_name] = X_val_final[col].apply(lambda x: frequency_map.get(x, 0))\n",
    "    X_test_final[new_col_name] = X_test_final[col].apply(lambda x: frequency_map.get(x, 0))\n",
    "\n",
    "    print(f\"Created '{new_col_name}' in all dataframes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original categorical columns after frequency encoding\n",
    "X_train_final = X_train_final.drop(columns=['Brand', 'model', 'transmission', 'mileage', 'fuelType', 'brand_model'])\n",
    "X_val_final = X_val_final.drop(columns=['Brand', 'model', 'transmission', 'mileage', 'fuelType', 'brand_model'])\n",
    "X_test_final = X_test_final.drop(columns=['Brand', 'model', 'transmission', 'mileage', 'fuelType', 'brand_model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"sec210-data-scaling\" \n",
    "     style=\"background-color:#644712c5; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px #644712c5;\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: rgba(242, 226, 9, 1);\"><b>2.10 | Data Scaling</b></span>\n",
    "    </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Robust Scaling**\n",
    "\n",
    "*RobustScaler* transforms the data by removing the median and scaling it according to the interquartile range (IQR = Q3 − Q1).  \n",
    "This means each feature is centered around the median and scaled by a range that covers the middle 50% of the data.\n",
    "\n",
    "**Advantages:**\n",
    "- **Resistant to outliers:** Unlike StandardScaler or MinMaxScaler, it is not influenced by extreme values since it uses the median and IQR instead of the mean and standard deviation.  \n",
    "- **Better for skewed data:** Works well when features have long tails or non-Gaussian distributions.  \n",
    "- **Maintains relative feature magnitude:** Scaling is still comparable across features, keeping proportional relationships.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Less effective on normally distributed data:** If there are no outliers, StandardScaler often performs better.  \n",
    "- **May compress useful variance:** Because it focuses on the central 50% of data, extreme but informative values can be downweighted.  \n",
    "- **Interpretability:** Transformed values are harder to interpret compared to original scales.\n",
    "\n",
    "**In summary:**  \n",
    "Robust scaling is ideal when data contains significant outliers or heavy-tailed distributions.  \n",
    "<span style=\"color:red;\">Tendo em conta esta explicaçao do sera que faz sentido manter este?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_train_final = pd.DataFrame(scaler.fit_transform(X_train_final), columns=X_train_final.columns, index=X_train_final.index)\n",
    "X_val_final = pd.DataFrame(scaler.transform(X_val_final), columns=X_val_final.columns, index=X_val_final.index)\n",
    "X_test_final = pd.DataFrame(scaler.transform(X_test_final), columns=X_test_final.columns, index=X_test_final.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#e5c120ff; padding:1px; border-radius:10px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed datasets to compressed CSV files\n",
    "X_train_final.to_csv(\"../project_data/X_train_final.csv.gz\", compression=\"gzip\")\n",
    "X_val_final.to_csv(\"../project_data/X_val_final.csv.gz\", compression=\"gzip\")\n",
    "X_test_final.to_csv(\"../project_data/X_test_final.csv.gz\", compression=\"gzip\")\n",
    "\n",
    "y_train.to_csv(\"../project_data/y_train.csv.gz\", compression=\"gzip\")\n",
    "y_val.to_csv(\"../project_data/y_val.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color: #ffffff;\">3 | Regression Benchmarking</span>\n",
    "\n",
    "<div style=\"background-color:#e5c120ff; padding:15px; border-radius:10px; \n",
    "            box-shadow: 0px 4px 12px rgba(227, 167, 108, 1);\">\n",
    "    <h1 style=\"margin:0; color:white; font-family:sans-serif; font-size:24px;\">\n",
    "         <span style=\"color: #644712ff;\"><b>3 | Regression Benchmarking</b></span>\n",
    "    </h1>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<div style=\"background-color:#e4b3c2ff; border:1.5px solid #e4b3c2ff; border-radius:8px; padding: 10px; width:1130px; text-align: justify;\">\n",
    "\n",
    "- Explanation of model assessment strategy and metrics used\n",
    "- Feature Selection Strategy and results\n",
    "- Optimization efforts: presentation, results and discussion\n",
    "- Comparison of performance between candidate models\n",
    "<br><br>\n",
    " - Identify the type of problem and select the relevant algorithms.\n",
    " - Select one model assessment strategy to use throughout your work. Which metrics are you using to evaluate your model and why?\n",
    " - Train at least 1 model using the train dataset and obtain predictions for the test dataset.(Extra 1 point) Be on the Top-5 Best Groups in the Kaggle Competition\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "# # Model definitions\n",
    "# models = {\n",
    "#     \"LinearRegression\": LinearRegression(),\n",
    "#     \"Ridge\": Ridge(),\n",
    "#     \"Lasso\": Lasso(),\n",
    "#     \"RandomForest\": RandomForestRegressor(),\n",
    "#     \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "#     \"DecisionTree\": DecisionTreeRegressor(),\n",
    "#     \"KNeighbors\": KNeighborsRegressor()\n",
    "# }\n",
    "\n",
    "# # Dictionary to store results\n",
    "# results = {}\n",
    "\n",
    "# # Iterate over each model\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"Training {model_name}...\")\n",
    "    \n",
    "#     # Train the model\n",
    "#     model.fit(X_train_enc, y_train) \n",
    "    \n",
    "#     # Predict on validation set\n",
    "#     y_pred = model.predict(X_val_enc)\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     mae = mean_absolute_error(y_val, y_pred)\n",
    "#     mse = mean_squared_error(y_val, y_pred)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     r2 = r2_score(y_val, y_pred)\n",
    "    \n",
    "#     # Store results\n",
    "#     results[model_name] = {\n",
    "#         \"MAE\": mae,\n",
    "#         \"MSE\": mse,\n",
    "#         \"RMSE\": rmse,\n",
    "#         \"R2\": r2\n",
    "#     }\n",
    "    \n",
    "#     # Print results\n",
    "#     print(f\"{model_name} MAE: {mae:.4f}\")\n",
    "#     print(f\"{model_name} RMSE: {rmse:.4f}\")\n",
    "#     print(f\"{model_name} R²: {r2:.4f}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "# # Print summary of all models\n",
    "# print(\"\\nModel Evaluation Results Summary:\")\n",
    "# for model_name, metrics in results.items():\n",
    "#     print(f\"{model_name}: MAE = {metrics['MAE']:.4f}, RMSE = {metrics['RMSE']:.4f}, R² = {metrics['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sem criar nem tirar variáveis (sem feature selection), usando OHE, sem tratar inconsistências, sem tratar outliers, sem fazer hyperparameter tuning\n",
    "# tratando missing values com média, usando StandardScaler\n",
    "# Model Evaluation Results Summary:\n",
    "# LinearRegression: MAE = 2554.7321, RMSE = 4177.1965, R² = 0.8204\n",
    "# Ridge: MAE = 2553.7875, RMSE = 4175.5577, R² = 0.8206\n",
    "# Lasso: MAE = 2551.7477, RMSE = 4173.1808, R² = 0.8208\n",
    "# RandomForest: MAE = 1482.2024, RMSE = 2499.0898, R² = 0.9357\n",
    "# GradientBoosting: MAE = 2321.1029, RMSE = 3500.0157, R² = 0.8739\n",
    "# DecisionTree: MAE = 1941.1537, RMSE = 3275.1219, R² = 0.8896\n",
    "# KNeighbors: MAE = 2384.9863, RMSE = 4107.6665, R² = 0.8264\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "# + tirar hasDamage e ver inconsistências em year\n",
    "# Model Evaluation Results Summary:\n",
    "# LinearRegression: MAE = 2554.5123, RMSE = 4174.7889, R² = 0.8206\n",
    "# Ridge: MAE = 2553.6070, RMSE = 4173.2138, R² = 0.8208\n",
    "# Lasso: MAE = 2551.6108, RMSE = 4170.9067, R² = 0.8210\n",
    "# RandomForest: MAE = 1483.6105, RMSE = 2516.0255, R² = 0.9349\n",
    "# GradientBoosting: MAE = 2321.9590, RMSE = 3489.0630, R² = 0.8747\n",
    "# DecisionTree: MAE = 1952.7124, RMSE = 3365.4139, R² = 0.8834\n",
    "# KNeighbors: MAE = 2384.8373, RMSE = 4107.8976, R² = 0.8263\n",
    "#não houve grandes alterações, em alguns modelos melhorou, noutros piorou\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# tratando das inconsistências das minhas variáveis + ter hasdamage\n",
    "# Model Evaluation Results Summary:\n",
    "# LinearRegression: MAE = 2549.0179, RMSE = 4170.6681, R² = 0.8210\n",
    "# Ridge: MAE = 2548.0386, RMSE = 4169.0990, R² = 0.8211\n",
    "# Lasso: MAE = 2546.5207, RMSE = 4167.1186, R² = 0.8213\n",
    "# RandomForest: MAE = 1466.3328, RMSE = 2485.5036, R² = 0.9364\n",
    "# GradientBoosting: MAE = 2307.7618, RMSE = 3451.5030, R² = 0.8774\n",
    "# DecisionTree: MAE = 1942.4345, RMSE = 3364.6791, R² = 0.8835\n",
    "# KNeighbors: MAE = 2106.1056, RMSE = 3683.4455, R² = 0.8604\n",
    "# melhorou até bastante\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA QUANDO FORMOS VER CORRELAÇÕES JÁ TEMOS ESTES INSIGHTS\n",
    "# It is peculiar that enginesize is slightly negatively correlated with mpg\n",
    "# Tax and engingesize positively correlated as expected\n",
    "# Tax and mpg negatively (contrary to what expected), same as for eginesize vs mpg\n",
    "# Price and tax positively correlated as expected (extra tax above a threshold of price aka luxury tax)\n",
    "## Nonetheless, the below are negigible correlation values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Fall2526",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
