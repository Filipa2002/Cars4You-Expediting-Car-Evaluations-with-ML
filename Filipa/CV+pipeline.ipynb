{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "    .header-banner {\n",
    "        background-color: white;\n",
    "        color: black; \n",
    "        padding: 1rem; \n",
    "        font-family: 'Nunito', sans-serif;\n",
    "    }\n",
    "    .header-content {\n",
    "        max-width: 2000px;\n",
    "        margin: 0 auto;\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "        gap: 2rem;\n",
    "    }\n",
    "    .logo {\n",
    "        max-width: 160px;\n",
    "    }\n",
    "    .text-content {\n",
    "        flex: 1;\n",
    "    }\n",
    "    .text-content h1 {\n",
    "        font-size: 34px;\n",
    "        margin: 0 0 10px;\n",
    "        font-weight: 700;\n",
    "        color: #7e4d02ff;\n",
    "        border-bottom: 2px solid #e5c120ff;\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "    .text-content h2 {\n",
    "        font-size: 21px;\n",
    "        margin: 0 0 5px;\n",
    "        font-weight: 600;\n",
    "        color: #222;\n",
    "    }\n",
    "    .member-list {\n",
    "        display: grid;\n",
    "        grid-template-columns: repeat(2, auto);\n",
    "        gap: 6px 40px;\n",
    "        font-size: 17px;\n",
    "        color: #444;\n",
    "    }\n",
    "    .member {\n",
    "        position: relative;\n",
    "        padding-left: 20px;\n",
    "    }\n",
    "</style>\n",
    "<link rel=\"preconnect\" href=\"https://fonts.googleapis.com\">\n",
    "<link rel=\"preconnect\" href=\"https://fonts.gstatic.com\" crossorigin>\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Nunito:wght@400;700&display=swap\" rel=\"stylesheet\">\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<header class=\"header-banner\">\n",
    "    <div class=\"header-content\">\n",
    "        <img src=\"https://i.ibb.co/JBPWVYR/Logo-Nova-IMS-Black.png\" alt=\"NOVA IMS Logo\" class=\"logo\">\n",
    "        <div class=\"text-content\">\n",
    "            <h1>Cars 4 You: Expediting Car Evaluations with ML</h1>\n",
    "            <h2>Group 37</h2>\n",
    "            <div class=\"member-list\">\n",
    "                <div class=\"member\">Filipa Pereira, 20240509</div>\n",
    "                <div class=\"member\">Gonçalo Silva, 20250354</div>\n",
    "                <div class=\"member\">Marta La Feria, 20211051 </div>\n",
    "                <div class=\"member\">Tomás Coroa, 20250394 </div>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</header>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# For models\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# For grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# For plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both learning and testing datasets\n",
    "learning = pd.read_csv('../project_data/train.csv') #Let's call Learning the Training + Validation sets together\n",
    "X_test = pd.read_csv('../project_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Feature Engineering Function\n",
    "def clean_and_prepare_data(df, is_learning_set=True):\n",
    "    print(f\"\\n*** Starting Data Cleaning for {'Learning' if is_learning_set else 'Test'} Set ***\")\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "\n",
    "    # 1. Set 'carID' as Index if unique\n",
    "    if 'carID' in df.columns:\n",
    "        if df['carID'].is_unique:\n",
    "            print(\"'carID' is unique so it will be set as index.\")\n",
    "            df.set_index('carID', inplace=True)\n",
    "        else:\n",
    "            print(\"'carID' is not unique. Cannot be set as index.\")\n",
    "            # If not unique then drop it\n",
    "            df.drop(columns=['carID'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    # 2. Deal with inconsistencies in categorical variables\n",
    "    categorical_cols_to_clean = ['Brand', 'transmission', 'fuelType']\n",
    "    # A simple mapping for known typos in 'Brand'\n",
    "    brand_corrections = {'bmw': 'BMW', 'vw': 'Volkswagen', 'merc': 'Mercedes-Benz', 'mw': 'BMW'}\n",
    "    for col in categorical_cols_to_clean:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.lower().str.strip() # Lowercase and remove whitespace\n",
    "            if col == 'Brand':\n",
    "                 df[col] = df[col].replace(brand_corrections)\n",
    "\n",
    "    print(\"Standardized 'Brand', 'transmission', and 'fuelType' columns.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #4. Clean and Validate Specific Variables\n",
    "    current_year = datetime.datetime.now().year\n",
    "\n",
    "    # 4.1. Year and previousOwners\n",
    "    if 'year' in df.columns:\n",
    "        df['year'] = df['year'].apply(lambda x: abs(int(x)) if pd.notnull(x) else x)\n",
    "        # Correct values outside the range [current_year-30, current_year]\n",
    "        df['year'] = df['year'].clip(lower=current_year - 30, upper=current_year)\n",
    "\n",
    "    if 'previousOwners' in df.columns:\n",
    "        df['previousOwners'] = df['previousOwners'].apply(lambda x: abs(int(x)) if pd.notnull(x) else x)\n",
    "\n",
    "    # 4.2. paintQuality%\n",
    "    if 'paintQuality%' in df.columns:\n",
    "        df['paintQuality%'] = df['paintQuality%'].clip(0, 100)\n",
    "\n",
    "    # 4.3. Other numeric variables\n",
    "    numeric_cols_to_abs = ['mileage', 'tax', 'mpg', 'engineSize']\n",
    "    for col in numeric_cols_to_abs:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: abs(float(x)) if pd.notnull(x) else np.nan)\n",
    "\n",
    "    print(\"Cleaned and validated numerical columns.\")\n",
    "\n",
    "\n",
    "\n",
    "    if is_learning_set:\n",
    "        # 1.1. Drop exact duplicates\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        print(f\"Shape after dropping full duplicates: {df.shape}\")\n",
    "        \n",
    "        # 1.2. Drop duplicates ignoring 'carID'\n",
    "        subset_cols = list(df.columns)\n",
    "        subset_cols.remove('carID')\n",
    "        df.drop_duplicates(subset=subset_cols, inplace=True)\n",
    "        print(f\"Shape after dropping duplicates (ignoring carID): {df.shape}\")\n",
    "    \n",
    "        # 1.3. Drop duplicates ignoring 'carID' and 'price'\n",
    "        subset_cols.remove('price')\n",
    "        df.drop_duplicates(subset=subset_cols, inplace=True)\n",
    "        print(f\"Shape after dropping duplicates (ignoring carID and price): {df.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    # 5. Feature Engineering\n",
    "    # 5.1. Car Age\n",
    "    if 'year' in df.columns:\n",
    "        df['car_age'] = current_year - df['year']\n",
    "        df.drop(columns=['year'], inplace=True) # Drop original year column\n",
    "\n",
    "    # 5.2. Brand and Model concatenation\n",
    "    if 'Brand' in df.columns and 'model' in df.columns:\n",
    "        df['brand_model'] = df['Brand'].astype(str) + '_' + df['model'].astype(str)\n",
    "        # We can keep original columns as they might be useful on their own\n",
    "        \n",
    "    print(\"Created new features: 'car_age' and 'brand_model'.\")\n",
    "\n",
    "    # --- 6. Drop Rows with High Percentage of Missing Values ---\n",
    "    min_non_missing = int(np.ceil(df.shape[1] * (1 - 0.4))) # At least 60% non-missing\n",
    "    rows_before_drop = df.shape[0]\n",
    "    df.dropna(thresh=min_non_missing, inplace=True)\n",
    "    rows_after_drop = df.shape[0]\n",
    "    print(f\"Dropped {rows_before_drop - rows_after_drop} rows with >= 40% missing values.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- 7. Handle Outliers using IQR method ---\n",
    "    ###APAGAR### This should be done after cleaning and imputation might be a better strategy in the pipeline\n",
    "    # But as requested, here is a method to cap them.\n",
    "    numeric_features = df.select_dtypes(include=np.number).columns.tolist()\n",
    "    if 'price' in numeric_features and is_learning_set:\n",
    "        numeric_features.remove('price') # Don't cap the target variable\n",
    "\n",
    "    for col in numeric_features:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "    print(\"Capped outliers in numerical columns using the IQR method.\")\n",
    "    print(f\"Final shape after cleaning: {df.shape}\")\n",
    "    print(\"--- End of Data Cleaning ---\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Data Cleaning for Learning Set ---\n",
      "Initial shape: (75973, 14)\n",
      "Shape after dropping full duplicates: (75973, 14)\n",
      "Shape after dropping duplicates (ignoring carID): (75969, 14)\n",
      "Shape after dropping duplicates (ignoring carID and price): (75962, 14)\n",
      "'carID' is unique in the learning set. Setting it as index.\n",
      "Standardized 'Brand', 'transmission', and 'fuelType' columns.\n",
      "Cleaned and validated numerical columns.\n",
      "Created new features: 'car_age' and 'brand_model'.\n",
      "Dropped 0 rows with >= 40% missing values.\n",
      "Capped outliers in numerical columns using the IQR method.\n",
      "Final shape after cleaning: (75962, 14)\n",
      "--- End of Data Cleaning ---\n",
      "\n",
      "--- Starting Data Cleaning for Test Set ---\n",
      "Initial shape: (32567, 13)\n",
      "Standardized 'Brand', 'transmission', and 'fuelType' columns.\n",
      "Cleaned and validated numerical columns.\n",
      "Created new features: 'car_age' and 'brand_model'.\n",
      "Dropped 1 rows with >= 40% missing values.\n",
      "Capped outliers in numerical columns using the IQR method.\n",
      "Final shape after cleaning: (32566, 13)\n",
      "--- End of Data Cleaning ---\n",
      "\n",
      "\u001b[1mUnique values for each column (up to 100) after cleaning:\u001b[0m\n",
      "Brand (33): ['Volkswagen' 'toyota' 'audi' 'ford' 'BMW' 'skoda' 'opel' 'mercedes' 'for'\n",
      " 'hyundai' 'w' 'ord' nan 'yundai' 'bm' 'toyot' 'udi' 'ope' 'v' 'pel' 'pe'\n",
      " 'mercede' 'koda' 'hyunda' 'aud' 'ercedes' 'oyota' 'skod' 'kod' 'yunda'\n",
      " 'or' 'ercede' 'ud']\n",
      "\n",
      "model (736): [More than 100 unique values, not shown]\n",
      "\n",
      "price (11048): [More than 100 unique values, not shown]\n",
      "\n",
      "transmission (18): ['semi-auto' 'manual' 'anual' 'semi-aut' 'automatic' nan 'unknown' 'manua'\n",
      " 'emi-auto' 'automati' 'utomatic' 'unknow' 'anua' 'emi-aut' 'nknown'\n",
      " 'other' 'utomati' 'nknow']\n",
      "\n",
      "mileage (33155): [More than 100 unique values, not shown]\n",
      "\n",
      "fuelType (17): ['petrol' 'diesel' 'etrol' 'hybrid' 'iesel' nan 'diese' 'petro' 'ybri'\n",
      " 'other' 'hybri' 'ybrid' 'electric' 'ther' 'iese' 'etro' 'othe']\n",
      "\n",
      "tax (15): [ nan 145.  95. 150. 160. 125. 175. 135. 165. 140. 155. 120. 115. 130.\n",
      " 110.]\n",
      "\n",
      "mpg (161): [More than 100 unique values, not shown]\n",
      "\n",
      "engineSize (40): [2.         1.5        1.         1.4        1.6               nan\n",
      " 1.2        3.         1.8        2.1        1.3        0.7395339\n",
      " 0.         2.3        1.1        3.2        1.7        2.98579344\n",
      " 2.2        2.5        2.61964142 0.1034928  2.78177621 2.63120047\n",
      " 0.15452932 2.9        2.8        0.31340658 2.06411863 2.4\n",
      " 0.18642488 2.6631132  0.19146242 0.30707394 0.37112737 1.9\n",
      " 0.6        0.16286595 2.7        2.6       ]\n",
      "\n",
      "paintQuality% (81): [ 63.          50.          56.          97.          74.\n",
      "  75.          59.          85.          35.          68.\n",
      "  45.          30.          69.          53.          57.\n",
      "  40.          60.          94.          33.          51.\n",
      "  47.          87.          61.          83.          95.\n",
      "  46.          54.          80.          77.          90.\n",
      "  91.          71.          44.          38.          64.\n",
      "  55.          66.          82.          41.          81.\n",
      "  79.          72.          34.          88.                  nan\n",
      "  43.          31.          32.          96.          84.\n",
      "  70.          86.          92.          52.          39.\n",
      "  42.          73.          78.          37.          65.\n",
      "  93.          89.          99.          36.          62.\n",
      " 100.           3.1152953   67.          76.          48.\n",
      "   3.17268306  49.           3.25476013  98.          58.\n",
      "   3.20741784   3.14037046   1.76947364   3.22574362   2.72515272\n",
      "   1.63891309]\n",
      "\n",
      "previousOwners (7): [ 4.  1.  2.  3.  0. nan  6.]\n",
      "\n",
      "hasDamage (2): [ 0. nan]\n",
      "\n",
      "car_age (14): [ 9.   6.   7.  11.   8.   5.  12.   nan 10.   2.  13.5 13.   1.5  3. ]\n",
      "\n",
      "brand_model (1351): [More than 100 unique values, not shown]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the cleaning function to both datasets\n",
    "learning_cleaned = clean_and_prepare_data(learning, is_learning_set=True)\n",
    "test_cleaned = clean_and_prepare_data(X_test, is_learning_set=False)\n",
    "\n",
    "# Inspect Unique Values\n",
    "print(\"\\n\\033[1mUnique values for each column (up to 100) after cleaning:\\033[0m\")\n",
    "# Let's inspect the learning set. Test set should be similar.\n",
    "for col in learning_cleaned.columns:\n",
    "    unique_vals = learning_cleaned[col].unique()\n",
    "    num_unique = len(unique_vals)\n",
    "    if num_unique > 100:\n",
    "        print(f\"{col} ({num_unique}): [More than 100 unique values, not shown]\\n\")\n",
    "    else:\n",
    "        print(f\"{col} ({num_unique}): {unique_vals}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and target variable from cleaned learning set\n",
    "X_learning = learning_cleaned.drop('price', axis=1)\n",
    "y_learning = learning_cleaned['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align columns - very important!\n",
    "# The test set might not have the same columns as the learning set after one-hot encoding\n",
    "# We will align them after fitting the preprocessor\n",
    "X_test = test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "###APAGAR### Só para tornar simples este primeiro modelo\n",
    "X_learning = X_learning.drop(['model', 'transmission'], axis=1)\n",
    "X_test = X_test.drop(['model', 'transmission'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns: ['mileage', 'tax', 'mpg', 'engineSize', 'paintQuality%', 'previousOwners', 'hasDamage', 'car_age']\n",
      "Categorical columns: ['Brand', 'fuelType', 'brand_model']\n",
      "\n",
      "==================== Running GridSearchCV for Ridge ====================\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "\n",
      "Results for Ridge:\n",
      "Best parameters found: {'regressor__alpha': 1.0}\n",
      "Best CV RMSE score: 4091.4736\n",
      "\n",
      "==================== Running GridSearchCV for RandomForest ====================\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Results for RandomForest:\n",
      "Best parameters found: {'regressor__max_depth': 20, 'regressor__min_samples_split': 5, 'regressor__n_estimators': 200}\n",
      "Best CV RMSE score: 2755.4158\n",
      "\n",
      "==================== Running GridSearchCV for GradientBoosting ====================\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "\n",
      "Results for GradientBoosting:\n",
      "Best parameters found: {'regressor__learning_rate': 0.1, 'regressor__max_depth': 5, 'regressor__n_estimators': 200}\n",
      "Best CV RMSE score: 2987.7328\n",
      "\n",
      "==================== Overall Best Model: RandomForest ====================\n",
      "Best overall CV RMSE: 2755.4158\n",
      "\n",
      "Making predictions on the submission test data using the best model...\n",
      "Predictions generated successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the preprocessing pipeline\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_features = X_learning.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X_learning.select_dtypes(exclude=np.number).columns.tolist()\n",
    "print(\"Numeric columns:\", numeric_features)\n",
    "print(\"Categorical columns:\", categorical_features)\n",
    "\n",
    "# Pipeline for numerical data\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Pipeline for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing for numeric and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define models and their parameter grids\n",
    "models_to_test = {\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'regressor__alpha': [0.1, 1.0, 10.0, 100.0, 200.0]\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(random_state=37),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': [100, 200],\n",
    "            'regressor__max_depth': [10, 20],\n",
    "            'regressor__min_samples_split': [2, 5]\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=37),\n",
    "        'params': {\n",
    "            'regressor__n_estimators': [100, 200],\n",
    "            'regressor__learning_rate': [0.05, 0.1],\n",
    "            'regressor__max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "best_score = -np.inf\n",
    "best_estimator = None\n",
    "best_model_name = \"\"\n",
    "\n",
    "\n",
    "# Iterate through each model, create a pipeline, and run GridSearchCV\n",
    "for name, info in models_to_test.items():\n",
    "    print(f\"\\n{'='*20} Running GridSearchCV for {name} {'='*20}\")\n",
    "\n",
    "    # Create the full pipeline with the current model\n",
    "    full_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', info['model'])\n",
    "    ])\n",
    "\n",
    "    # Create and run GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        full_pipeline,\n",
    "        info['params'],\n",
    "        cv=5,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_learning, y_learning)\n",
    "\n",
    "    print(f\"\\nResults for {name}:\")\n",
    "    print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "    # The score is negative, so we negate it to make it positive RMSE\n",
    "    print(f\"Best CV RMSE score: {-grid_search.best_score_:.4f}\")\n",
    "\n",
    "    # Check if this model is the best one so far\n",
    "    if grid_search.best_score_ > best_score:\n",
    "        best_score = grid_search.best_score_\n",
    "        best_estimator = grid_search.best_estimator_\n",
    "        best_model_name = name\n",
    "\n",
    "print(f\"\\n{'='*20} Overall Best Model: {best_model_name} {'='*20}\")\n",
    "print(f\"Best overall CV RMSE: {-best_score:.4f}\")\n",
    "\n",
    "# --- Make Final Predictions ---\n",
    "print(\"\\nMaking predictions on the submission test data using the best model...\")\n",
    "# Ensure test set columns are in the same order as training set before predicting\n",
    "# The pipeline handles this internally, but it's good practice\n",
    "final_predictions = best_estimator.predict(X_test)\n",
    "\n",
    "print(\"Predictions generated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOMCV É MELHOR\n",
    "\n",
    "# Quando você chama grid_search.fit(X_learning, y_learning), o GridSearchCV pega nos dados de treino e validação (X_learning) e executa o processo de validação cruzada internamente.\n",
    "# O parâmetro cv=5 instrui o GridSearchCV a fazer o seguinte:\n",
    "# Ignorar X_test: Ele só vai olhar para os dados que lhe foram passados: X_learning.\n",
    "# Dividir em 5 \"Folds\": Ele divide X_learning em 5 pedaços de tamanho aproximadamente igual.\n",
    "# [Fold 1 | Fold 2 | Fold 3 | Fold 4 | Fold 5]\n",
    "# Iterar 5 Vezes: Para cada combinação de hiperparâmetros que você definiu no param_grid, ele vai repetir o seguinte processo 5 vezes:\n",
    "# Corrida 1: Treina o modelo usando os Folds 2, 3, 4 e 5. Depois, valida (mede a pontuação) no Fold 1.\n",
    "# Corrida 2: Treina o modelo usando os Folds 1, 3, 4 e 5. Depois, valida no Fold 2.\n",
    "# Corrida 3: Treina o modelo usando os Folds 1, 2, 4 e 5. Depois, valida no Fold 3.\n",
    "# Corrida 4: Treina o modelo usando os Folds 1, 2, 3 e 5. Depois, valida no Fold 4.\n",
    "# Corrida 5: Treina o modelo usando os Folds 1, 2, 3 e 4. Depois, valida no Fold 5.\n",
    "# Calcular a Média: No final destas 5 corridas, ele calcula a média das 5 pontuações de validação. Este valor médio é a \"pontuação de validação cruzada\" (CV score) para aquela combinação de hiperparâmetros.\n",
    "# Encontrar o Melhor: O GridSearchCV faz isto para todas as combinações ({'C': 0.01, 'penalty': 'l1'}, {'C': 0.01, 'penalty': 'l2'}, etc.). No final, ele compara todas as pontuações médias e determina qual a combinação de hiperparâmetros que teve o melhor desempenho.\n",
    "# Treino Final: Depois de encontrar os melhores hiperparâmetros, o GridSearchCV automaticamente treina um novo modelo, do zero, usando todos os dados de X_train_val com esses hiperparâmetros ótimos. Este modelo final é o que fica guardado em grid_search.best_estimator_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALTERNATIVAS\n",
    "# POSSIVELMENTE DEIXAR UNKNOWN E PÔR OS MISSINGS A UNKNOWN OU PREENCHER COM A MODA/MEDIAN OU COM KNN\n",
    "# paintQuality%<0 talvez usar abs antes de clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O que fiz até agora:\n",
    "# -  Set Index<- carID\n",
    "# - inconsistencies de variáveis categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O que tenho ainda que fazer:\n",
    "# - apresentar CRISP-DM, flowchart (gráficos+img), index (se necessário), papers e referências sobre tudo o que não demos, requirements no github\n",
    "# - comentar todo o código e justificar (por exemplo dizer que fizémos CV...), abstract e group members contributions\n",
    "# - ver qual o erro cometido no test\n",
    "# - ver projetos dos anos anteriores\n",
    "# ------\n",
    "# - outliers\n",
    "# - missings\n",
    "# - inconsistências\n",
    "# - duplicados\n",
    "# - feature selection\n",
    "# - medidas de performance\n",
    "# - EDA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
